{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a1795cc",
   "metadata": {},
   "source": [
    "Market Efficiency Analysis for Prediction Markets\n",
    "----------------------------------------------\n",
    "This notebook analyzes the efficiency of prediction markets using various statistical tests to evaluate\n",
    "whether these markets follow the \"wisdom of crowds\" hypothesis.\n",
    "## 1. Introduction & Research Questions\n",
    "\n",
    "This analysis aims to answer the following research questions:\n",
    "1. Do prediction markets on Polymarket exhibit weak-form efficiency?\n",
    "2. How does efficiency vary across different market types and contexts?\n",
    "3. Does efficiency change over a market's lifecycle?\n",
    "4. Can one market predict price movements in related markets?\n",
    "\n",
    "Efficient markets should have the following characteristics:\n",
    "- Non-stationary price series (random walk)\n",
    "- Stationary return series\n",
    "- No significant autocorrelation in returns\n",
    "- No significant predictability through AR models\n",
    "- Variance ratios close to 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eef993",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0973f190",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "from statsmodels.tsa.stattools import acf, pacf, adfuller\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "import warnings\n",
    "import json\n",
    "from scipy import stats\n",
    "\n",
    "# Try to use the notebook progress bar, fall back to terminal version if not available\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "except ImportError:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"viridis\")\n",
    "\n",
    "# Add the src directory to the path if it isn't already there\n",
    "if '../src' not in sys.path:\n",
    "    sys.path.append('../src')\n",
    "\n",
    "# Import utility functions\n",
    "from src.utils.data_loader import load_main_dataset, load_trade_data, get_sample_market_ids, load_market_question_mapping\n",
    "\n",
    "# Create output directory for saving results\n",
    "results_dir = 'results/knowledge_value/efficiency'\n",
    "os.makedirs(results_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71194dc",
   "metadata": {},
   "source": [
    " ## 3. Load and Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeff4d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Load the main dataset\n",
    "print(\"Loading main dataset...\")\n",
    "main_df = load_main_dataset('data/cleaned_election_data.csv')\n",
    "print(f\"Loaded dataset with {main_df.shape[0]} rows and {main_df.shape[1]} columns\")\n",
    "\n",
    "# Create a custom market questions mapping directly from the DataFrame\n",
    "market_questions = dict(zip(main_df['id'], main_df['question']))\n",
    "print(f\"Created mapping for {len(market_questions)} markets\")\n",
    "\n",
    "# %%\n",
    "# Check column names to understand available data\n",
    "print(\"\\nColumn names in the dataset:\")\n",
    "print(main_df.columns.tolist())\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Determine ID Column for Markets\n",
    "\n",
    "# %%\n",
    "# Determine ID column\n",
    "id_column = None\n",
    "if 'market_id' in main_df.columns:\n",
    "    id_column = 'market_id'\n",
    "elif 'id' in main_df.columns:\n",
    "    id_column = 'id'\n",
    "else:\n",
    "    # Use the first column as ID\n",
    "    id_column = main_df.columns[0]\n",
    "    print(f\"Using {id_column} as the ID column\")\n",
    "\n",
    "print(f\"Using {id_column} as market identifier column\")\n",
    "\n",
    "# %%\n",
    "# Display some sample data\n",
    "print(\"\\nSample data:\")\n",
    "display(main_df.head())\n",
    "\n",
    "# %%\n",
    "# Get distribution of market types\n",
    "if 'event_electionType' in main_df.columns:\n",
    "    print(\"\\nDistribution of market types:\")\n",
    "    display(main_df['event_electionType'].value_counts())\n",
    "\n",
    "# %%\n",
    "# Get distribution by region/country\n",
    "if 'event_country' in main_df.columns:\n",
    "    print(\"\\nDistribution by country:\")\n",
    "    display(main_df['event_country'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16e1b99",
   "metadata": {},
   "source": [
    "### 3.1 Select Markets for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31042c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of market IDs for analysis\n",
    "sort_column = 'volumeNum' if 'volumeNum' in main_df.columns else id_column\n",
    "sample_markets = main_df.sort_values(sort_column, ascending=False)[id_column].unique()\n",
    "\n",
    "# Limit the number of markets for initial analysis\n",
    "analysis_markets = sample_markets[:10]  # Adjust based on your computational resources\n",
    "print(f\"\\nSelected {len(analysis_markets)} markets for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8972dcf",
   "metadata": {},
   "source": [
    "## 4. Methodology & Implementation\n",
    "\n",
    "### 4.1 Market Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb383b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_market_data(market_id, resample='1min'):\n",
    "    \"\"\"\n",
    "    Convert raw trade data to time series of prices and returns.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    market_id : str\n",
    "        The ID of the market to analyze\n",
    "    resample : str\n",
    "        Frequency to resample the time series (default: '1min')\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with columns: timestamp, price, log_return\n",
    "    \"\"\"\n",
    "    # Load trade data for the specific market\n",
    "    trades_df = load_trade_data(market_id, trades_dir=\"data/trades\")\n",
    "    \n",
    "    if trades_df is None or len(trades_df) < 30:\n",
    "        print(f\"Insufficient trade data for market {market_id}\")\n",
    "        return None\n",
    "    \n",
    "    # Ensure timestamp is a datetime type\n",
    "    if not pd.api.types.is_datetime64_any_dtype(trades_df['timestamp']):\n",
    "        # First convert to numeric to avoid FutureWarning\n",
    "        if pd.api.types.is_string_dtype(trades_df['timestamp']):\n",
    "            numeric_timestamps = pd.to_numeric(trades_df['timestamp'], errors='coerce')\n",
    "            trades_df['timestamp'] = pd.to_datetime(numeric_timestamps, unit='s')\n",
    "        else:\n",
    "            trades_df['timestamp'] = pd.to_datetime(trades_df['timestamp'], unit='s')\n",
    "    \n",
    "    # Sort by timestamp\n",
    "    trades_df = trades_df.sort_values('timestamp')\n",
    "    \n",
    "    # Ensure price is numeric - find the right column\n",
    "    price_col = None\n",
    "    for col in ['price', 'price_num']:\n",
    "        if col in trades_df.columns:\n",
    "            trades_df[col] = pd.to_numeric(trades_df[col], errors='coerce')\n",
    "            price_col = col\n",
    "            break\n",
    "    \n",
    "    if price_col is None:\n",
    "        print(f\"No price column found for market {market_id}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Market ID: {market_id}\")\n",
    "    print(f\"Original trades count: {len(trades_df)}\")\n",
    "    print(f\"Price column: {trades_df['price'].describe()}\")\n",
    "    print(f\"Timestamp range: {trades_df['timestamp'].min()} to {trades_df['timestamp'].max()}\")\n",
    "    \n",
    "    \n",
    "\n",
    "    # Drop rows with NaN prices\n",
    "    trades_df = trades_df.dropna(subset=[price_col])\n",
    "    \n",
    "    # Use price_col for consistency\n",
    "    if price_col != 'price':\n",
    "        trades_df['price'] = trades_df[price_col]\n",
    "    \n",
    "    # Resample to regular intervals - this step can be expensive for large datasets\n",
    "    # Set the index once and avoid unnecessary operations\n",
    "    trades_df = trades_df.set_index('timestamp')\n",
    "    \n",
    "    # Only keep the price column to reduce memory usage\n",
    "    price_series = trades_df['price']\n",
    "    \n",
    "    # Use efficient resampling with forward fill\n",
    "    price_series = price_series.resample(resample).last().ffill()\n",
    "    \n",
    "    # Check for adequate data after resampling\n",
    "    if len(price_series) < 10:\n",
    "        print(f\"Insufficient data after resampling for market {market_id}\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate log returns - avoid duplicate calculations\n",
    "    log_returns = np.log(price_series / price_series.shift(1))\n",
    "    \n",
    "    # Create DataFrame efficiently\n",
    "    result_df = pd.DataFrame({\n",
    "        'price': price_series,\n",
    "        'log_return': log_returns\n",
    "    })\n",
    "    \n",
    "    # Drop NaN values (first row will have NaN log return)\n",
    "    result_df = result_df.dropna()\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Create a cache for market data\n",
    "market_data_cache = {}\n",
    "\n",
    "def preprocess_market_data_cached(market_id, resample='1min'):\n",
    "    \"\"\"\n",
    "    Cached version of preprocess_market_data that stores results for reuse\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    market_id : str\n",
    "        The ID of the market to analyze\n",
    "    resample : str\n",
    "        Frequency to resample the time series (default: '1min')\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with columns: timestamp, price, log_return\n",
    "    \"\"\"\n",
    "    # Normalize market_id to ensure consistent cache keys\n",
    "    cache_key = (str(market_id), resample)\n",
    "    \n",
    "    # Check if already in cache\n",
    "    if cache_key in market_data_cache:\n",
    "        return market_data_cache[cache_key]\n",
    "    \n",
    "    # If not in cache, process and store\n",
    "    result = preprocess_market_data(market_id, resample)\n",
    "    if result is not None:\n",
    "        market_data_cache[cache_key] = result\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb46f2cd",
   "metadata": {},
   "source": [
    "### 4.2 Test a Single Market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43aa1577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_market_details(market_id, main_df, market_questions):\n",
    "    \"\"\"\n",
    "    Retrieve detailed market information\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    market_id : str or int\n",
    "        Market identifier\n",
    "    main_df : pd.DataFrame\n",
    "        Main dataset containing market information\n",
    "    market_questions : dict\n",
    "        Mapping of market IDs to questions\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary of market details\n",
    "    \"\"\"\n",
    "    # Retrieve question directly using market ID\n",
    "    question = market_questions.get(market_id, 'Unknown Question')\n",
    "    \n",
    "    market_details = {\n",
    "        'id': market_id,\n",
    "        'question': question,\n",
    "        'event_type': 'Unknown',\n",
    "        'country': 'Unknown',\n",
    "        'volume': 0,\n",
    "        'duration_days': 0\n",
    "    }\n",
    "    \n",
    "    # Try to find market details in the main dataframe\n",
    "    try:\n",
    "        # Convert market_id to string for comparison\n",
    "        market_row = main_df[main_df[id_column].astype(str) == str(market_id)]\n",
    "        \n",
    "        if not market_row.empty:\n",
    "            market_row = market_row.iloc[0]\n",
    "            \n",
    "            # Try to get additional details from the dataset\n",
    "            detail_mappings = [\n",
    "                ('event_electionType', 'event_type'),\n",
    "                ('event_country', 'country'),\n",
    "                ('volumeNum', 'volume'),\n",
    "                ('market_duration_days', 'duration_days')\n",
    "            ]\n",
    "            \n",
    "            for source_col, detail_key in detail_mappings:\n",
    "                if source_col in market_row.index and not pd.isna(market_row[source_col]):\n",
    "                    market_details[detail_key] = market_row[source_col]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving market details: {e}\")\n",
    "    \n",
    "    return market_details\n",
    "\n",
    "def test_market_preprocessing(analysis_markets, main_df, market_questions):\n",
    "    \"\"\"\n",
    "    Efficiently test market preprocessing with comprehensive information\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    analysis_markets : list\n",
    "        List of market IDs to test\n",
    "    main_df : pd.DataFrame\n",
    "        Main dataset containing market information\n",
    "    market_questions : dict\n",
    "        Mapping of market IDs to questions\n",
    "    \"\"\"\n",
    "    # Try multiple markets until successful preprocessing\n",
    "    for market_id in analysis_markets:\n",
    "        market_details = get_market_details(market_id, main_df, market_questions)\n",
    "        \n",
    "        print(\"\\n🔍 Detailed Market Information\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Market ID: {market_id}\")\n",
    "        print(f\"Event Question: {market_details['question']}\")\n",
    "        print(f\"Event Type: {market_details['event_type']}\")\n",
    "        print(f\"Country: {market_details['country']}\")\n",
    "        print(f\"Trading Volume: {market_details['volume']:,}\")\n",
    "        print(f\"Market Duration: {market_details['duration_days']} days\")\n",
    "        \n",
    "        try:\n",
    "            market_data = preprocess_market_data(market_id)\n",
    "            \n",
    "            if market_data is not None:\n",
    "                print(f\"\\n✅ Successfully processed market data with {len(market_data)} rows\")\n",
    "                \n",
    "                # Descriptive statistics\n",
    "                print(\"\\n📊 Price Series Statistics:\")\n",
    "                print(market_data['price'].describe())\n",
    "                \n",
    "                # Use plt.subplots for efficient plotting\n",
    "                fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))\n",
    "                \n",
    "                # Price series\n",
    "                ax1.plot(market_data.index, market_data['price'], color='blue')\n",
    "                ax1.set_title(f'Price Series: {market_details[\"question\"]}')\n",
    "                ax1.set_xlabel('Date')\n",
    "                ax1.set_ylabel('Price')\n",
    "                ax1.grid(True, alpha=0.3)\n",
    "                \n",
    "                # Return series\n",
    "                ax2.plot(market_data.index, market_data['log_return'], color='green')\n",
    "                ax2.set_title(f'Log Return Series: {market_details[\"question\"]}')\n",
    "                ax2.set_xlabel('Date')\n",
    "                ax2.set_ylabel('Log Return')\n",
    "                ax2.grid(True, alpha=0.3)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "                #break  # Stop after first successful market\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing market {market_id}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    else:\n",
    "        print(\"❌ No markets could be processed. Check data loading and preprocessing.\")\n",
    "\n",
    "# Usage\n",
    "test_market_preprocessing(analysis_markets, main_df, market_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecea7b6",
   "metadata": {},
   "source": [
    "\n",
    "### 4.3 Define Efficiency Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b8ad0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_adf_test(series, series_type='price'):\n",
    "    \"\"\"\n",
    "    Run Augmented Dickey-Fuller test for unit root.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    series : pd.Series\n",
    "        Time series to test\n",
    "    series_type : str\n",
    "        Type of series ('price' or 'return')\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with test results\n",
    "    \"\"\"\n",
    "    # Run ADF test\n",
    "    result = adfuller(series.dropna())\n",
    "    \n",
    "    # Format results\n",
    "    adf_result = {\n",
    "        'adf_statistic': result[0],\n",
    "        'pvalue': result[1],\n",
    "        'critical_values': result[4],\n",
    "        'is_stationary': result[1] < 0.05  # Reject unit root if p-value < 0.05\n",
    "    }\n",
    "    \n",
    "    return adf_result\n",
    "\n",
    "def run_autocorrelation_tests(returns, lags=10):\n",
    "    \"\"\"\n",
    "    Run ACF tests on return series.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    returns : pd.Series\n",
    "        Series of log returns\n",
    "    lags : int\n",
    "        Number of lags to test\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with ACF results and significance\n",
    "    \"\"\"\n",
    "    # Calculate ACF\n",
    "    acf_values = acf(returns, nlags=lags, fft=True)\n",
    "    \n",
    "    # Calculate significance threshold\n",
    "    significance_level = 1.96 / np.sqrt(len(returns))  # 95% confidence level\n",
    "    \n",
    "    # Check for significant autocorrelation\n",
    "    significant_lags = []\n",
    "    for i in range(1, len(acf_values)):  # Skip lag 0 (always 1)\n",
    "        if abs(acf_values[i]) > significance_level:\n",
    "            significant_lags.append(i)\n",
    "    \n",
    "    result = {\n",
    "        'acf_values': acf_values.tolist(),\n",
    "        'significant_lags': significant_lags,\n",
    "        'has_significant_autocorrelation': len(significant_lags) > 0\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "def run_variance_ratio_test(returns, periods=[1, 5, 15, 60]):\n",
    "    \"\"\"\n",
    "    Run variance ratio test to check if variance scales linearly with time.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    returns : pd.Series\n",
    "        Series of log returns\n",
    "    periods : list\n",
    "        List of periods to test\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with variance ratio results\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Calculate variance for base period\n",
    "    base_period = periods[0]\n",
    "    base_var = returns.var()\n",
    "    \n",
    "    for period in periods[1:]:\n",
    "        # Skip if we don't have enough data\n",
    "        if len(returns) < period * 10:\n",
    "            continue\n",
    "            \n",
    "        # Aggregate returns for longer period\n",
    "        agg_returns = returns.rolling(window=period).sum()\n",
    "        agg_returns = agg_returns.dropna()\n",
    "        \n",
    "        if len(agg_returns) <= 1:\n",
    "            continue\n",
    "            \n",
    "        # Calculate variance\n",
    "        period_var = agg_returns.var()\n",
    "        \n",
    "        # Calculate variance ratio\n",
    "        var_ratio = period_var / (period * base_var)\n",
    "        \n",
    "        # Random walk hypothesis: var_ratio should be close to 1\n",
    "        # Calculate z-statistic (simplified)\n",
    "        n = len(returns)\n",
    "        std_error = np.sqrt(2 * (2 * period - 1) * (period - 1) / (3 * period * n))\n",
    "        z_stat = (var_ratio - 1) / std_error\n",
    "        p_value = 2 * (1 - abs(np.exp(-0.5 * z_stat**2) / np.sqrt(2 * np.pi)))\n",
    "        \n",
    "        results[f\"{period}min\"] = {\n",
    "            'variance_ratio': var_ratio,\n",
    "            'z_statistic': z_stat,\n",
    "            'p_value': p_value,\n",
    "            'significant': p_value < 0.05,\n",
    "            'interpretation': 'Mean Reversion' if var_ratio < 1 else 'Momentum' if var_ratio > 1 else 'Random Walk'\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def run_runs_test(returns):\n",
    "    \"\"\"\n",
    "    Run a runs test to check for non-random patterns in returns.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    returns : pd.Series\n",
    "        Series of log returns\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with runs test results\n",
    "    \"\"\"\n",
    "    # Convert returns to binary sequence (1 for positive, 0 for negative)\n",
    "    binary_seq = (returns > 0).astype(int)\n",
    "    \n",
    "    # Count runs\n",
    "    runs = 1\n",
    "    for i in range(1, len(binary_seq)):\n",
    "        if binary_seq.iloc[i] != binary_seq.iloc[i-1]:  # Use iloc for positional indexing\n",
    "            runs += 1\n",
    "    \n",
    "    # Calculate expected runs and variance\n",
    "    n = len(binary_seq)\n",
    "    n1 = binary_seq.sum()  # Count of 1s\n",
    "    n0 = n - n1  # Count of 0s\n",
    "    \n",
    "    if n0 == 0 or n1 == 0:  # All returns are positive or negative\n",
    "        return {\n",
    "            'runs': runs,\n",
    "            'expected_runs': np.nan,\n",
    "            'z_statistic': np.nan,\n",
    "            'p_value': np.nan,\n",
    "            'is_random': False\n",
    "        }\n",
    "    \n",
    "    expected_runs = 1 + 2 * n1 * n0 / n\n",
    "    std_runs = np.sqrt(2 * n1 * n0 * (2 * n1 * n0 - n) / (n**2 * (n-1)))\n",
    "    \n",
    "    # Calculate z-statistic\n",
    "    z_stat = (runs - expected_runs) / std_runs\n",
    "    p_value = 2 * (1 - abs(np.exp(-0.5 * z_stat**2) / np.sqrt(2 * np.pi)))\n",
    "    \n",
    "    return {\n",
    "        'runs': runs,\n",
    "        'expected_runs': expected_runs,\n",
    "        'z_statistic': z_stat,\n",
    "        'p_value': p_value,\n",
    "        'is_random': p_value >= 0.05  # Null hypothesis is randomness\n",
    "    }\n",
    "\n",
    "def fit_ar_model(returns, lags=1):\n",
    "    \"\"\"\n",
    "    Fit AR model to return series and evaluate predictability.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    returns : pd.Series\n",
    "        Series of log returns\n",
    "    lags : int\n",
    "        Order of the AR model\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with model results\n",
    "    \"\"\"\n",
    "    if len(returns) <= lags + 2:\n",
    "        return None\n",
    "        \n",
    "    # Fit AR model\n",
    "    try:\n",
    "        model = AutoReg(returns, lags=lags)\n",
    "        model_fit = model.fit()\n",
    "        \n",
    "        # Extract coefficient and p-value\n",
    "        coef = model_fit.params[1] if len(model_fit.params) > 1 else 0\n",
    "        p_value = model_fit.pvalues[1] if len(model_fit.pvalues) > 1 else 1\n",
    "        \n",
    "        return {\n",
    "            'ar_coefficient': coef,\n",
    "            'p_value': p_value,\n",
    "            'significant': p_value < 0.05,\n",
    "            'aic': model_fit.aic,\n",
    "            'bic': model_fit.bic\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error fitting AR model: {e}\")\n",
    "        return None\n",
    "\n",
    "def analyze_time_varying_efficiency(returns):\n",
    "    \"\"\"\n",
    "    Analyze how efficiency changes over time by dividing the returns series \n",
    "    into early, middle, and late periods.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    returns : pd.Series\n",
    "        Series of log returns\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with time-varying efficiency results\n",
    "    \"\"\"\n",
    "    if len(returns) < 90:  # Need enough data to divide\n",
    "        return None\n",
    "    \n",
    "    # Divide into three periods\n",
    "    period_size = len(returns) // 3\n",
    "    early_returns = returns.iloc[:period_size]\n",
    "    mid_returns = returns.iloc[period_size:2*period_size]\n",
    "    late_returns = returns.iloc[2*period_size:]\n",
    "    \n",
    "    # Test each period\n",
    "    periods = {\n",
    "        'early': early_returns,\n",
    "        'middle': mid_returns,\n",
    "        'late': late_returns\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for period_name, period_returns in periods.items():\n",
    "        if len(period_returns) < 30:  # Skip if not enough data\n",
    "            continue\n",
    "        \n",
    "        # Calculate autocorrelation\n",
    "        acf_result = run_autocorrelation_tests(period_returns)\n",
    "        \n",
    "        # Fit AR model\n",
    "        ar_result = fit_ar_model(period_returns)\n",
    "        \n",
    "        results[period_name] = {\n",
    "            'significant_acf': acf_result.get('has_significant_autocorrelation', False),\n",
    "            'ar_model': ar_result,\n",
    "            'return_volatility': period_returns.std(),\n",
    "            'sample_size': len(period_returns)\n",
    "        }\n",
    "    \n",
    "    # Compare early vs late\n",
    "    if 'early' in results and 'late' in results:\n",
    "        early_ar_sig = results['early'].get('ar_model', {}).get('significant', False) if results['early'].get('ar_model') else False\n",
    "        late_ar_sig = results['late'].get('ar_model', {}).get('significant', False) if results['late'].get('ar_model') else False\n",
    "        \n",
    "        efficiency_change = 'No Change'\n",
    "        if early_ar_sig and not late_ar_sig:\n",
    "            efficiency_change = 'More Efficient'\n",
    "        elif not early_ar_sig and late_ar_sig:\n",
    "            efficiency_change = 'Less Efficient'\n",
    "        \n",
    "        volatility_ratio = results['late']['return_volatility'] / results['early']['return_volatility'] if results['early']['return_volatility'] > 0 else 1\n",
    "        \n",
    "        results['comparison'] = {\n",
    "            'efficiency_change': efficiency_change,\n",
    "            'volatility_ratio': volatility_ratio,\n",
    "            'early_more_inefficient': early_ar_sig and not late_ar_sig,\n",
    "            'late_more_inefficient': not early_ar_sig and late_ar_sig\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_cross_market_predictability(market_ids, max_lag=3):\n",
    "    \"\"\"Test for Granger causality between related markets\"\"\"\n",
    "    if len(market_ids) < 2:\n",
    "        return None\n",
    "    \n",
    "    # Process each market\n",
    "    market_data = {}\n",
    "    for market_id in market_ids:\n",
    "        data = preprocess_market_data(market_id, resample='5min')  # Use wider intervals for cross-market\n",
    "        if data is not None and len(data) > max_lag + 5:\n",
    "            market_name = market_questions.get(str(market_id), f\"Market {market_id}\")\n",
    "            market_data[market_id] = {\n",
    "                'data': data,\n",
    "                'name': market_name\n",
    "            }\n",
    "    \n",
    "    if len(market_data) < 2:\n",
    "        return None\n",
    "    \n",
    "    # Pairwise Granger causality tests\n",
    "    causality_results = []\n",
    "    \n",
    "    for i, (market_i, data_i) in enumerate(market_data.items()):\n",
    "        for j, (market_j, data_j) in enumerate(market_data.items()):\n",
    "            if i >= j:  # Skip self-comparisons and duplicates\n",
    "                continue\n",
    "            \n",
    "            # Align time series\n",
    "            common_index = data_i['data'].index.intersection(data_j['data'].index)\n",
    "            if len(common_index) <= max_lag + 5:\n",
    "                continue\n",
    "                \n",
    "            series_i = data_i['data'].loc[common_index, 'price']\n",
    "            series_j = data_j['data'].loc[common_index, 'price']\n",
    "            \n",
    "            # Test if market i Granger-causes market j\n",
    "            try:\n",
    "                # i -> j\n",
    "                gc_result_ij = grangercausalitytests(\n",
    "                    pd.concat([series_j, series_i], axis=1), \n",
    "                    maxlag=max_lag, \n",
    "                    verbose=False\n",
    "                )\n",
    "                min_pvalue_ij = min([res[0]['ssr_chi2test'][1] for lag, res in gc_result_ij.items()])\n",
    "                \n",
    "                # j -> i\n",
    "                gc_result_ji = grangercausalitytests(\n",
    "                    pd.concat([series_i, series_j], axis=1), \n",
    "                    maxlag=max_lag, \n",
    "                    verbose=False\n",
    "                )\n",
    "                min_pvalue_ji = min([res[0]['ssr_chi2test'][1] for lag, res in gc_result_ji.items()])\n",
    "                \n",
    "                causality_results.append({\n",
    "                    'market_i_id': market_i,\n",
    "                    'market_j_id': market_j,\n",
    "                    'market_i_name': data_i['name'],\n",
    "                    'market_j_name': data_j['name'],\n",
    "                    'i_causes_j_pvalue': min_pvalue_ij,\n",
    "                    'j_causes_i_pvalue': min_pvalue_ji,\n",
    "                    'i_causes_j': min_pvalue_ij < 0.05,\n",
    "                    'j_causes_i': min_pvalue_ji < 0.05,\n",
    "                    'bidirectional': min_pvalue_ij < 0.05 and min_pvalue_ji < 0.05\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error in Granger causality test: {e}\")\n",
    "    \n",
    "    return causality_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22274f0",
   "metadata": {},
   "source": [
    "### 4.4 Run Tests on Single Market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa3550b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_market_efficiency_tests(market_id, market_details, market_data):\n",
    "    \"\"\"\n",
    "    Comprehensive market efficiency analysis with customizable reporting\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    market_id : int or float\n",
    "        Market identifier\n",
    "    market_details : dict\n",
    "        Market details dictionary\n",
    "    market_data : pd.DataFrame\n",
    "        Preprocessed market data\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Detailed efficiency test results\n",
    "    \"\"\"\n",
    "    # Ensure sufficient data\n",
    "    if market_data is None or len(market_data) <= 60:\n",
    "        print(f\"❌ Insufficient data for market {market_id}\")\n",
    "        return None\n",
    "    \n",
    "    # Prepare results dictionary\n",
    "    efficiency_results = {\n",
    "        'market_id': market_id,\n",
    "        'market_name': market_details.get('question', 'Unknown Market')\n",
    "    }\n",
    "    \n",
    "    # Efficiency Tests\n",
    "    try:\n",
    "        # 1. ADF Tests\n",
    "        efficiency_results['adf_price'] = run_adf_test(market_data['price'], 'price')\n",
    "        efficiency_results['adf_return'] = run_adf_test(market_data['log_return'], 'return')\n",
    "        \n",
    "        # 2. Autocorrelation Test\n",
    "        efficiency_results['autocorrelation'] = run_autocorrelation_tests(market_data['log_return'])\n",
    "        \n",
    "        # 3. Variance Ratio Test\n",
    "        efficiency_results['variance_ratio'] = run_variance_ratio_test(market_data['log_return'])\n",
    "        \n",
    "        # 4. Runs Test\n",
    "        efficiency_results['runs_test'] = run_runs_test(market_data['log_return'])\n",
    "        \n",
    "        # 5. AR Model\n",
    "        efficiency_results['ar_model'] = fit_ar_model(market_data['log_return'])\n",
    "        \n",
    "        # 6. Time-Varying Efficiency\n",
    "        efficiency_results['time_varying'] = analyze_time_varying_efficiency(market_data['log_return'])\n",
    "        \n",
    "        # Visualization\n",
    "        visualize_market_efficiency(market_data, market_details)\n",
    "        \n",
    "        return efficiency_results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error analyzing market {market_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "def visualize_market_efficiency(market_data, market_details):\n",
    "    \"\"\"\n",
    "    Create visualizations for market efficiency analysis\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    market_data : pd.DataFrame\n",
    "        Preprocessed market data\n",
    "    market_details : dict\n",
    "        Market details dictionary\n",
    "    \"\"\"\n",
    "    # Create a single figure with multiple subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 15))\n",
    "    \n",
    "    # 1. Price Series\n",
    "    axes[0, 0].plot(market_data.index, market_data['price'], color='blue')\n",
    "    axes[0, 0].set_title(f'Price Series: {market_details.get(\"question\", \"Market Price\")}')\n",
    "    axes[0, 0].set_xlabel('Date')\n",
    "    axes[0, 0].set_ylabel('Price')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Log Returns Series\n",
    "    axes[0, 1].plot(market_data.index, market_data['log_return'], color='green')\n",
    "    axes[0, 1].set_title(f'Log Returns: {market_details.get(\"question\", \"Market Returns\")}')\n",
    "    axes[0, 1].set_xlabel('Date')\n",
    "    axes[0, 1].set_ylabel('Log Return')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Price Distribution\n",
    "    sns.histplot(market_data['price'], kde=True, ax=axes[1, 0], color='blue')\n",
    "    axes[1, 0].set_title('Price Distribution')\n",
    "    axes[1, 0].set_xlabel('Price')\n",
    "    \n",
    "    # 4. Returns Distribution\n",
    "    sns.histplot(market_data['log_return'], kde=True, ax=axes[1, 1], color='green')\n",
    "    axes[1, 1].set_title('Log Returns Distribution')\n",
    "    axes[1, 1].set_xlabel('Log Return')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f'Market Efficiency Analysis: {market_details.get(\"question\", \"Unknown Market\")}', fontsize=16)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18abdc36",
   "metadata": {},
   "source": [
    "### 4.5 Representative Market Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4e5bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_market_efficiency_sample(top_n_markets=10, by_volume=True, verbose=False):\n",
    "    \"\"\"\n",
    "    Analyze a representative sample of markets for efficiency testing\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    top_n_markets : int\n",
    "        Number of markets to analyze\n",
    "    by_volume : bool\n",
    "        Whether to select markets by volume (True) or randomly (False)\n",
    "    verbose : bool\n",
    "        Whether to print detailed information\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with efficiency results for the sample markets\n",
    "    \"\"\"\n",
    "    global main_df, id_column\n",
    "    \n",
    "    # Select the sample markets\n",
    "    if by_volume:\n",
    "        # Sort by volume if available\n",
    "        sort_column = 'volumeNum' if 'volumeNum' in main_df.columns else id_column\n",
    "        sample_markets = main_df.sort_values(sort_column, ascending=False)[id_column].unique()[:top_n_markets]\n",
    "        sample_type = \"highest volume\"\n",
    "    else:\n",
    "        # Random sample\n",
    "        sample_markets = main_df[id_column].sample(top_n_markets).values\n",
    "        sample_type = \"random\"\n",
    "    \n",
    "    print(f\"Analyzing {sample_type} sample of {len(sample_markets)} markets...\")\n",
    "    \n",
    "    # Process each market\n",
    "    results = []\n",
    "    for market_id in sample_markets:\n",
    "        try:\n",
    "            market_result = {'market_id': market_id}\n",
    "            \n",
    "            # Get market name\n",
    "            market_name = None\n",
    "            try:\n",
    "                market_rows = main_df[main_df[id_column] == market_id]\n",
    "                if len(market_rows) > 0 and 'question' in market_rows.columns:\n",
    "                    market_name = market_rows.iloc[0]['question']\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            if market_name is None:\n",
    "                try:\n",
    "                    # Try to get from market_questions mapping\n",
    "                    market_name = market_questions.get(str(market_id), f\"Market {market_id}\")\n",
    "                except:\n",
    "                    market_name = f\"Market {market_id}\"\n",
    "            \n",
    "            market_result['market_name'] = market_name\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"\\nAnalyzing market: {market_name} (ID: {market_id})\")\n",
    "            \n",
    "            # Preprocess market data\n",
    "            market_data = preprocess_market_data_cached(market_id, verbose=verbose)\n",
    "            if market_data is None:\n",
    "                if verbose:\n",
    "                    print(f\"Skipping market {market_id}: insufficient data\")\n",
    "                continue\n",
    "            \n",
    "            # Get market metadata\n",
    "            market_rows = main_df[main_df[id_column] == market_id]\n",
    "            if len(market_rows) == 0:\n",
    "                # Try string comparison\n",
    "                market_rows = main_df[main_df[id_column].astype(str) == str(market_id)]\n",
    "            \n",
    "            if len(market_rows) > 0:\n",
    "                row = market_rows.iloc[0]\n",
    "                \n",
    "                # Extract market information safely\n",
    "                for col, result_key in [\n",
    "                    ('event_electionType', 'event_type'),\n",
    "                    ('event_country', 'country'),\n",
    "                    ('volumeNum', 'volume'),\n",
    "                    ('market_duration_days', 'duration_days')\n",
    "                ]:\n",
    "                    if col in row and not pd.isna(row[col]):\n",
    "                        market_result[result_key] = row[col]\n",
    "            \n",
    "            # Run efficiency tests\n",
    "            # 1. ADF tests\n",
    "            market_result['adf_price'] = run_adf_test(market_data['price'], 'price')\n",
    "            market_result['adf_return'] = run_adf_test(market_data['log_return'], 'return')\n",
    "            \n",
    "            # 2. Autocorrelation tests\n",
    "            market_result['autocorrelation'] = run_autocorrelation_tests(market_data['log_return'])\n",
    "            \n",
    "            # 3. Variance ratio test\n",
    "            market_result['variance_ratio'] = run_variance_ratio_test(market_data['log_return'])\n",
    "            \n",
    "            # 4. Runs test\n",
    "            market_result['runs_test'] = run_runs_test(market_data['log_return'])\n",
    "            \n",
    "            # 5. AR model\n",
    "            market_result['ar_model'] = fit_ar_model(market_data['log_return'])\n",
    "            \n",
    "            # 6. Time-varying efficiency analysis\n",
    "            market_result['time_varying'] = analyze_time_varying_efficiency(market_data['log_return'])\n",
    "            \n",
    "            # Calculate efficiency score\n",
    "            efficiency_score = calculate_efficiency_score(market_result)\n",
    "            market_result['efficiency_score'] = efficiency_score\n",
    "            \n",
    "            # Classification\n",
    "            if efficiency_score >= 80:\n",
    "                market_result['efficiency_class'] = 'Highly Efficient'\n",
    "            elif efficiency_score >= 60:\n",
    "                market_result['efficiency_class'] = 'Moderately Efficient'\n",
    "            elif efficiency_score >= 40:\n",
    "                market_result['efficiency_class'] = 'Slightly Inefficient'\n",
    "            else:\n",
    "                market_result['efficiency_class'] = 'Highly Inefficient'\n",
    "            \n",
    "            results.append(market_result)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"Completed analysis for {market_name} - Efficiency Score: {efficiency_score:.2f}/100\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing market {market_id}: {e}\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Print summary information\n",
    "    print(\"\\nAnalysis complete!\")\n",
    "    print(f\"Successfully analyzed {len(results_df)}/{len(sample_markets)} markets\")\n",
    "    if len(results_df) > 0:\n",
    "        print(f\"Average efficiency score: {results_df['efficiency_score'].mean():.2f}/100\")\n",
    "        print(\"\\nEfficiency Classification:\")\n",
    "        for cls, count in results_df['efficiency_class'].value_counts().items():\n",
    "            print(f\"  {cls}: {count} markets ({count/len(results_df)*100:.1f}%)\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def calculate_efficiency_score(market_result):\n",
    "    \"\"\"Calculate an efficiency score based on various test results\"\"\"\n",
    "    score = 0\n",
    "    max_points = 0\n",
    "    \n",
    "    # 1. Non-stationary price (random walk) = efficient\n",
    "    if 'adf_price' in market_result:\n",
    "        max_points += 1\n",
    "        if not market_result['adf_price']['is_stationary']:\n",
    "            score += 1\n",
    "    \n",
    "    # 2. Stationary returns = efficient\n",
    "    if 'adf_return' in market_result:\n",
    "        max_points += 1\n",
    "        if market_result['adf_return']['is_stationary']:\n",
    "            score += 1\n",
    "    \n",
    "    # 3. No significant autocorrelation = efficient\n",
    "    if 'autocorrelation' in market_result:\n",
    "        max_points += 1\n",
    "        if not market_result['autocorrelation']['has_significant_autocorrelation']:\n",
    "            score += 1\n",
    "    \n",
    "    # 4. Random runs test = efficient\n",
    "    if 'runs_test' in market_result:\n",
    "        max_points += 1\n",
    "        if market_result['runs_test']['is_random']:\n",
    "            score += 1\n",
    "    \n",
    "    # 5. No significant AR model = efficient\n",
    "    if 'ar_model' in market_result and market_result['ar_model']:\n",
    "        max_points += 1\n",
    "        if not market_result['ar_model']['significant']:\n",
    "            score += 1\n",
    "    \n",
    "    # 6. Variance ratio close to 1 = efficient\n",
    "    if 'variance_ratio' in market_result and market_result['variance_ratio']:\n",
    "        vr_count = 0\n",
    "        vr_score = 0\n",
    "        for period, result in market_result['variance_ratio'].items():\n",
    "            max_points += 0.5\n",
    "            vr_count += 0.5\n",
    "            if not result['significant']:  # Not significantly different from 1\n",
    "                score += 0.5\n",
    "                vr_score += 0.5\n",
    "    \n",
    "    # Calculate percentage\n",
    "    if max_points > 0:\n",
    "        efficiency_score = (score / max_points) * 100\n",
    "    else:\n",
    "        efficiency_score = 0\n",
    "    \n",
    "    return efficiency_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03538e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_market_efficiency_results(results_df, save_dir=None):\n",
    "    \"\"\"\n",
    "    Create visualizations of market efficiency results\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results_df : pd.DataFrame\n",
    "        DataFrame with efficiency results\n",
    "    save_dir : str, optional\n",
    "        Directory to save plots\n",
    "    \"\"\"\n",
    "    if save_dir:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # 1. Efficiency score distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(results_df['efficiency_score'], bins=20, kde=True)\n",
    "    plt.axvline(x=results_df['efficiency_score'].mean(), color='red', linestyle='--', \n",
    "               label=f'Mean: {results_df[\"efficiency_score\"].mean():.2f}')\n",
    "    plt.title('Distribution of Market Efficiency Scores', fontsize=14)\n",
    "    plt.xlabel('Efficiency Score (higher = more efficient)', fontsize=12)\n",
    "    plt.ylabel('Count', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    if save_dir:\n",
    "        plt.savefig(os.path.join(save_dir, 'efficiency_score_distribution.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Efficiency classification pie chart\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    results_df['efficiency_class'].value_counts().plot.pie(autopct='%1.1f%%', \n",
    "                                                         colors=sns.color_palette(\"viridis\", 4),\n",
    "                                                         startangle=90, \n",
    "                                                         textprops={'fontsize': 12})\n",
    "    plt.title('Market Efficiency Classification', fontsize=14)\n",
    "    plt.ylabel('')  # Hide ylabel\n",
    "    \n",
    "    if save_dir:\n",
    "        plt.savefig(os.path.join(save_dir, 'efficiency_classification_pie.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # 3. Test results summary\n",
    "    test_results = []\n",
    "    \n",
    "    if 'adf_price' in results_df.columns:\n",
    "        price_stationary = results_df['adf_price'].apply(lambda x: x['is_stationary'] if isinstance(x, dict) else False).mean() * 100\n",
    "        test_results.append(('Non-Stationary Prices', 100 - price_stationary))\n",
    "    \n",
    "    if 'adf_return' in results_df.columns:\n",
    "        return_stationary = results_df['adf_return'].apply(lambda x: x['is_stationary'] if isinstance(x, dict) else False).mean() * 100\n",
    "        test_results.append(('Stationary Returns', return_stationary))\n",
    "    \n",
    "    if 'autocorrelation' in results_df.columns:\n",
    "        no_autocorr = 100 - results_df['autocorrelation'].apply(\n",
    "            lambda x: x['has_significant_autocorrelation'] if isinstance(x, dict) else False).mean() * 100\n",
    "        test_results.append(('No Significant Autocorrelation', no_autocorr))\n",
    "    \n",
    "    if 'runs_test' in results_df.columns:\n",
    "        random_runs = results_df['runs_test'].apply(lambda x: x['is_random'] if isinstance(x, dict) else False).mean() * 100\n",
    "        test_results.append(('Random Runs Test', random_runs))\n",
    "    \n",
    "    if 'ar_model' in results_df.columns:\n",
    "        no_sig_ar = 100 - results_df['ar_model'].apply(\n",
    "            lambda x: x.get('significant', False) if isinstance(x, dict) else False).mean() * 100\n",
    "        test_results.append(('No Significant AR Model', no_sig_ar))\n",
    "    \n",
    "    # Plot if we have results\n",
    "    if test_results:\n",
    "        test_df = pd.DataFrame(test_results, columns=['Test', 'Pass Rate (%)'])\n",
    "        test_df = test_df.sort_values('Pass Rate (%)', ascending=False)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        bars = plt.barh(test_df['Test'], test_df['Pass Rate (%)'], color='skyblue')\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, bar in enumerate(bars):\n",
    "            plt.text(bar.get_width() + 1, bar.get_y() + bar.get_height()/2, \n",
    "                    f\"{bar.get_width():.1f}%\", ha='left', va='center', fontsize=11)\n",
    "        \n",
    "        plt.title('Efficiency Test Pass Rates', fontsize=14)\n",
    "        plt.xlabel('Pass Rate (%)', fontsize=12)\n",
    "        plt.xlim(0, 110)\n",
    "        plt.grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        if save_dir:\n",
    "            plt.savefig(os.path.join(save_dir, 'efficiency_test_results.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    # 4. Efficiency by market type if available\n",
    "    if 'event_type' in results_df.columns:\n",
    "        type_counts = results_df['event_type'].value_counts()\n",
    "        types_with_sufficient_data = type_counts[type_counts >= 2].index.tolist()\n",
    "        \n",
    "        if types_with_sufficient_data:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            \n",
    "            # Create a DataFrame with efficiency by type\n",
    "            type_efficiency = results_df[results_df['event_type'].isin(types_with_sufficient_data)].groupby('event_type')['efficiency_score'].agg(['mean', 'count']).sort_values('mean', ascending=False)\n",
    "            \n",
    "            bars = plt.bar(type_efficiency.index, type_efficiency['mean'], color='lightgreen')\n",
    "            \n",
    "            # Add count labels\n",
    "            for i, bar in enumerate(bars):\n",
    "                plt.text(bar.get_x() + bar.get_width()/2, \n",
    "                        bar.get_height() + 1, \n",
    "                        f\"n={type_efficiency['count'].iloc[i]}\", \n",
    "                        ha='center', va='bottom', fontsize=10)\n",
    "            \n",
    "            plt.axhline(y=results_df['efficiency_score'].mean(), color='red', linestyle='--', \n",
    "                       label=f'Overall Average: {results_df[\"efficiency_score\"].mean():.2f}')\n",
    "            \n",
    "            plt.title('Average Efficiency Score by Market Type', fontsize=14)\n",
    "            plt.xlabel('Market Type', fontsize=12)\n",
    "            plt.ylabel('Average Efficiency Score', fontsize=12)\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.legend()\n",
    "            plt.ylim(0, 100)\n",
    "            plt.grid(axis='y', alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            if save_dir:\n",
    "                plt.savefig(os.path.join(save_dir, 'efficiency_by_market_type.png'), dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "    \n",
    "    # 5. Time-varying efficiency results if available\n",
    "    time_varying_data = []\n",
    "    for _, row in results_df.iterrows():\n",
    "        tv = row.get('time_varying', {})\n",
    "        if isinstance(tv, dict) and 'comparison' in tv:\n",
    "            time_varying_data.append({\n",
    "                'market_id': row['market_id'],\n",
    "                'market_name': row.get('market_name', f\"Market {row['market_id']}\"),\n",
    "                'efficiency_change': tv['comparison'].get('efficiency_change', 'No Change'),\n",
    "                'volatility_ratio': tv['comparison'].get('volatility_ratio', 1)\n",
    "            })\n",
    "    \n",
    "    if time_varying_data:\n",
    "        tv_df = pd.DataFrame(time_varying_data)\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        tv_counts = tv_df['efficiency_change'].value_counts()\n",
    "        \n",
    "        # Use appropriate colors\n",
    "        colors = {'More Efficient': 'green', 'No Change': 'gray', 'Less Efficient': 'red'}\n",
    "        bar_colors = [colors.get(category, 'blue') for category in tv_counts.index]\n",
    "        \n",
    "        bars = plt.bar(tv_counts.index, tv_counts.values, color=bar_colors)\n",
    "        \n",
    "        # Add percentage labels\n",
    "        total = len(tv_df)\n",
    "        for i, count in enumerate(tv_counts):\n",
    "            plt.text(i, count + 0.5, f\"{count/total*100:.1f}%\", ha='center', fontsize=11)\n",
    "        \n",
    "        plt.title('Efficiency Change Over Market Lifecycle', fontsize=14)\n",
    "        plt.ylabel('Number of Markets', fontsize=12)\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        if save_dir:\n",
    "            plt.savefig(os.path.join(save_dir, 'time_varying_efficiency.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856a0bdf",
   "metadata": {},
   "source": [
    "## 5. Comprehensive Efficiency Analysis of All Markets\n",
    "\n",
    "### 5.1 Analysis Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70860c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_market_efficiency(market_id):\n",
    "    \"\"\"\n",
    "    Run a comprehensive market efficiency analysis on a single market.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    market_id : str\n",
    "        ID of the market to analyze\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with efficiency results\n",
    "    \"\"\"\n",
    "    market_result = {'market_id': market_id}\n",
    "    market_name = market_questions.get(str(market_id), f\"Market {market_id}\")\n",
    "    market_result['market_name'] = market_name\n",
    "    \n",
    "    # Preprocess market data using cached version\n",
    "    market_data = preprocess_market_data_cached(market_id)\n",
    "    if market_data is None or len(market_data) < 30:\n",
    "        return None\n",
    "    \n",
    "    # Get market metadata\n",
    "    market_rows = main_df[main_df[id_column] == market_id]\n",
    "    if len(market_rows) == 0:\n",
    "        # Try string comparison\n",
    "        market_rows = main_df[main_df[id_column].astype(str) == str(market_id)]\n",
    "    \n",
    "    if len(market_rows) > 0:\n",
    "        row = market_rows.iloc[0]\n",
    "        \n",
    "        # Extract market information safely\n",
    "        if 'event_electionType' in row:\n",
    "            market_result['event_type'] = row['event_electionType']\n",
    "        if 'event_country' in row:\n",
    "            market_result['country'] = row['event_country']\n",
    "        if 'volumeNum' in row:\n",
    "            market_result['volume'] = row['volumeNum']\n",
    "        if 'market_duration_days' in row:\n",
    "            market_result['duration_days'] = row['market_duration_days']\n",
    "    \n",
    "    # Run ADF tests\n",
    "    market_result['adf_price'] = run_adf_test(market_data['price'], 'price')\n",
    "    market_result['adf_return'] = run_adf_test(market_data['log_return'], 'return')\n",
    "    \n",
    "    # Run autocorrelation tests\n",
    "    market_result['autocorrelation'] = run_autocorrelation_tests(market_data['log_return'])\n",
    "    \n",
    "    # Run variance ratio test\n",
    "    market_result['variance_ratio'] = run_variance_ratio_test(market_data['log_return'])\n",
    "    \n",
    "    # Run runs test\n",
    "    market_result['runs_test'] = run_runs_test(market_data['log_return'])\n",
    "    \n",
    "    # Fit AR model\n",
    "    market_result['ar_model'] = fit_ar_model(market_data['log_return'])\n",
    "    \n",
    "    # Run time-varying efficiency analysis\n",
    "    market_result['time_varying'] = analyze_time_varying_efficiency(market_data['log_return'])\n",
    "    \n",
    "    # Calculate overall efficiency score (0-100, higher = more efficient)\n",
    "    score = 0\n",
    "    max_points = 0\n",
    "    \n",
    "    # 1. Non-stationary price (random walk) = efficient\n",
    "    if 'adf_price' in market_result:\n",
    "        max_points += 1\n",
    "        if not market_result['adf_price']['is_stationary']:\n",
    "            score += 1\n",
    "    \n",
    "    # 2. Stationary returns = efficient\n",
    "    if 'adf_return' in market_result:\n",
    "        max_points += 1\n",
    "        if market_result['adf_return']['is_stationary']:\n",
    "            score += 1\n",
    "    \n",
    "    # 3. No significant autocorrelation = efficient\n",
    "    if 'autocorrelation' in market_result:\n",
    "        max_points += 1\n",
    "        if not market_result['autocorrelation']['has_significant_autocorrelation']:\n",
    "            score += 1\n",
    "    \n",
    "    # 4. Random runs test = efficient\n",
    "    if 'runs_test' in market_result:\n",
    "        max_points += 1\n",
    "        if market_result['runs_test']['is_random']:\n",
    "            score += 1\n",
    "    \n",
    "    # 5. No significant AR model = efficient\n",
    "    if 'ar_model' in market_result and market_result['ar_model']:\n",
    "        max_points += 1\n",
    "        if not market_result['ar_model']['significant']:\n",
    "            score += 1\n",
    "    \n",
    "    # 6. Variance ratio close to 1 = efficient\n",
    "    if 'variance_ratio' in market_result and market_result['variance_ratio']:\n",
    "        vr_count = 0\n",
    "        for period, result in market_result['variance_ratio'].items():\n",
    "            max_points += 0.5\n",
    "            vr_count += 0.5\n",
    "            if not result['significant']:  # Not significantly different from 1\n",
    "                score += 0.5\n",
    "    \n",
    "    # Calculate percentage\n",
    "    if max_points > 0:\n",
    "        efficiency_score = (score / max_points) * 100\n",
    "    else:\n",
    "        efficiency_score = 0\n",
    "    \n",
    "    market_result['efficiency_score'] = efficiency_score\n",
    "    \n",
    "    # Efficiency classification\n",
    "    if efficiency_score >= 80:\n",
    "        market_result['efficiency_class'] = 'Highly Efficient'\n",
    "    elif efficiency_score >= 60:\n",
    "        market_result['efficiency_class'] = 'Moderately Efficient'\n",
    "    elif efficiency_score >= 40:\n",
    "        market_result['efficiency_class'] = 'Slightly Inefficient'\n",
    "    else:\n",
    "        market_result['efficiency_class'] = 'Highly Inefficient'\n",
    "    \n",
    "    market_result['market_question'] = market_questions.get(str(market_id), 'Unknown')\n",
    "    market_result['election_type'] = market_info.get('event_electionType', 'Unknown')\n",
    "    market_result['country'] = market_info.get('event_country', 'Unknown')\n",
    "    market_result['total_volume'] = market_info.get('volumeNum', 0)\n",
    "    market_result['market_duration_days'] = market_info.get('market_duration_days', 0)\n",
    "\n",
    "    return market_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3422bff",
   "metadata": {},
   "source": [
    "### 5.2 Run Analysis on All Selected Markets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07adb740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_markets_efficiently(market_ids, max_markets=100, max_workers=None):\n",
    "    \"\"\"\n",
    "    Run market efficiency analysis with improved performance and memory management\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    market_ids : list\n",
    "        List of market IDs to analyze\n",
    "    max_markets : int\n",
    "        Maximum number of markets to analyze\n",
    "    max_workers : int, optional\n",
    "        Number of parallel workers (defaults to CPU cores - 2)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        Processed market efficiency results\n",
    "    \"\"\"\n",
    "    from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "    import multiprocessing\n",
    "    \n",
    "    # Limit markets to reduce computational load\n",
    "    market_ids = market_ids[:max_markets]\n",
    "    \n",
    "    # Determine optimal number of workers\n",
    "    if max_workers is None:\n",
    "        max_workers = max(1, multiprocessing.cpu_count() - 2)\n",
    "    \n",
    "    results = []\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit jobs and collect results as they complete\n",
    "        futures = {executor.submit(analyze_market_efficiency, market_id): market_id \n",
    "                   for market_id in market_ids}\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result is not None:\n",
    "                results.append(result)\n",
    "                \n",
    "                # Optional: print progress\n",
    "                print(f\"Processed market {futures[future]} successfully\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17620ad3",
   "metadata": {},
   "source": [
    "### 5.3 Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64201c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_market_efficiency_results(results_df):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive summary of market efficiency results\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results_df : pd.DataFrame\n",
    "        DataFrame containing market efficiency results\n",
    "    \"\"\"\n",
    "    if results_df.empty:\n",
    "        print(\"No results to summarize.\")\n",
    "        return\n",
    "    \n",
    "    # Efficiency overview\n",
    "    print(\"\\n📊 Market Efficiency Analysis Summary\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Total Markets Analyzed: {len(results_df)}\")\n",
    "    print(f\"Average Efficiency Score: {results_df['efficiency_score'].mean():.2f}/100\")\n",
    "    \n",
    "    # Efficiency classification breakdown\n",
    "    classification_breakdown = results_df['efficiency_class'].value_counts(normalize=True) * 100\n",
    "    print(\"\\n🔍 Efficiency Classification:\")\n",
    "    for cls, percentage in classification_breakdown.items():\n",
    "        print(f\"  {cls}: {percentage:.1f}%\")\n",
    "    \n",
    "    # Detailed test results\n",
    "    efficiency_tests = {\n",
    "        'adf_price': ('Price Stationarity', lambda x: not x['is_stationary']),\n",
    "        'adf_return': ('Return Stationarity', lambda x: x['is_stationary']),\n",
    "        'autocorrelation': ('Significant Autocorrelation', lambda x: x['has_significant_autocorrelation']),\n",
    "        'runs_test': ('Random Runs', lambda x: x['is_random']),\n",
    "        'ar_model': ('Significant AR Model', lambda x: x.get('significant', False) if x else False)\n",
    "    }\n",
    "    \n",
    "    print(\"\\n📈 Detailed Test Results:\")\n",
    "    for column, (test_name, condition) in efficiency_tests.items():\n",
    "        if column in results_df.columns:\n",
    "            test_result = results_df[column].apply(condition).mean() * 100\n",
    "            print(f\"  {test_name}: {test_result:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4aea08",
   "metadata": {},
   "source": [
    "## 6. Data Visualization and Interpretation\n",
    "### 6.1 Distribution of Efficiency Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330acc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of efficiency scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(results_df['efficiency_score'], bins=20, kde=True)\n",
    "plt.axvline(x=results_df['efficiency_score'].mean(), color='red', linestyle='--', \n",
    "           label=f'Mean: {results_df[\"efficiency_score\"].mean():.2f}')\n",
    "plt.title('Distribution of Market Efficiency Scores')\n",
    "plt.xlabel('Efficiency Score (higher = more efficient)')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Efficiency classification pie chart\n",
    "plt.figure(figsize=(10, 8))\n",
    "results_df['efficiency_class'].value_counts().plot.pie(autopct='%1.1f%%', \n",
    "                                                     colors=sns.color_palette(\"viridis\", 4),\n",
    "                                                     startangle=90)\n",
    "plt.title('Market Efficiency Classification')\n",
    "plt.ylabel('')  # Hide ylabel\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5863cc",
   "metadata": {},
   "source": [
    "### 6.2 Efficiency by Market Characteristics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51711a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'event_type' in results_df.columns:\n",
    "    # Filter to include only market types with sufficient data\n",
    "    type_counts = results_df['event_type'].value_counts()\n",
    "    market_types = type_counts[type_counts >= 5].index.tolist()\n",
    "    \n",
    "    if market_types:\n",
    "        type_data = []\n",
    "        for market_type in market_types:\n",
    "            type_df = results_df[results_df['event_type'] == market_type]\n",
    "            type_data.append({\n",
    "                'Market Type': market_type,\n",
    "                'Average Efficiency': type_df['efficiency_score'].mean(),\n",
    "                'Count': len(type_df)\n",
    "            })\n",
    "        \n",
    "        type_df = pd.DataFrame(type_data).sort_values('Average Efficiency', ascending=False)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        bars = plt.bar(type_df['Market Type'], type_df['Average Efficiency'], color='skyblue')\n",
    "        \n",
    "        # Add count labels\n",
    "        for i, bar in enumerate(bars):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 1, \n",
    "                    f\"n={type_df['Count'].iloc[i]}\", \n",
    "                    ha='center', va='bottom')\n",
    "        \n",
    "        plt.axhline(y=results_df['efficiency_score'].mean(), color='red', linestyle='--', \n",
    "                   label=f'Overall Average: {results_df[\"efficiency_score\"].mean():.2f}')\n",
    "        \n",
    "        plt.title('Average Efficiency Score by Market Type')\n",
    "        plt.xlabel('Market Type')\n",
    "        plt.ylabel('Average Efficiency Score')\n",
    "        plt.legend()\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.ylim(0, 100)\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# %%\n",
    "# Efficiency by region/country\n",
    "if 'country' in results_df.columns:\n",
    "    # Filter to include only countries with sufficient data\n",
    "    country_counts = results_df['country'].value_counts()\n",
    "    countries = country_counts[country_counts >= 5].index.tolist()\n",
    "    \n",
    "    if countries:\n",
    "        country_data = []\n",
    "        for country in countries:\n",
    "            country_df = results_df[results_df['country'] == country]\n",
    "            country_data.append({\n",
    "                'Country': country,\n",
    "                'Average Efficiency': country_df['efficiency_score'].mean(),\n",
    "                'Count': len(country_df)\n",
    "            })\n",
    "        \n",
    "        country_df = pd.DataFrame(country_data).sort_values('Average Efficiency', ascending=False)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        bars = plt.bar(country_df['Country'], country_df['Average Efficiency'], color='lightgreen')\n",
    "        \n",
    "        # Add count labels\n",
    "        for i, bar in enumerate(bars):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 1, \n",
    "                    f\"n={country_df['Count'].iloc[i]}\", \n",
    "                    ha='center', va='bottom')\n",
    "        \n",
    "        plt.axhline(y=results_df['efficiency_score'].mean(), color='red', linestyle='--', \n",
    "                   label=f'Overall Average: {results_df[\"efficiency_score\"].mean():.2f}')\n",
    "        \n",
    "        plt.title('Average Efficiency Score by Country')\n",
    "        plt.xlabel('Country')\n",
    "        plt.ylabel('Average Efficiency Score')\n",
    "        plt.legend()\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.ylim(0, 100)\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4064d7",
   "metadata": {},
   "source": [
    "### 6.3 Efficiency vs Market Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e3bf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficiency vs Market Volume\n",
    "if 'volume' in results_df.columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(results_df['volume'], results_df['efficiency_score'], alpha=0.6)\n",
    "    plt.xscale('log')  # Use log scale for volume\n",
    "    plt.title('Efficiency Score vs Market Volume')\n",
    "    plt.xlabel('Trading Volume (log scale)')\n",
    "    plt.ylabel('Efficiency Score')\n",
    "    \n",
    "    # Add trend line\n",
    "    try:\n",
    "        z = np.polyfit(np.log10(results_df['volume']), results_df['efficiency_score'], 1)\n",
    "        p = np.poly1d(z)\n",
    "        plt.plot(sorted(results_df['volume']), \n",
    "                p(np.log10(sorted(results_df['volume']))), \n",
    "                \"r--\", linewidth=2)\n",
    "        \n",
    "        # Calculate correlation\n",
    "        corr = np.corrcoef(np.log10(results_df['volume']), results_df['efficiency_score'])[0, 1]\n",
    "        plt.text(0.05, 0.95, f\"Correlation: {corr:.3f}\", transform=plt.gca().transAxes)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# %%\n",
    "# Efficiency vs Market Duration\n",
    "if 'duration_days' in results_df.columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(results_df['duration_days'], results_df['efficiency_score'], alpha=0.6)\n",
    "    plt.title('Efficiency Score vs Market Duration')\n",
    "    plt.xlabel('Market Duration (days)')\n",
    "    plt.ylabel('Efficiency Score')\n",
    "    \n",
    "    # Add trend line\n",
    "    try:\n",
    "        z = np.polyfit(results_df['duration_days'], results_df['efficiency_score'], 1)\n",
    "        p = np.poly1d(z)\n",
    "        plt.plot(sorted(results_df['duration_days']), \n",
    "                p(sorted(results_df['duration_days'])), \n",
    "                \"r--\", linewidth=2)\n",
    "        \n",
    "        # Calculate correlation\n",
    "        corr = np.corrcoef(results_df['duration_days'], results_df['efficiency_score'])[0, 1]\n",
    "        plt.text(0.05, 0.95, f\"Correlation: {corr:.3f}\", transform=plt.gca().transAxes)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba363b01",
   "metadata": {},
   "source": [
    "### 6.4 Time-Varying Efficiency Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f674123b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze time-varying efficiency results\n",
    "tv_data = []\n",
    "\n",
    "for result in efficiency_results:\n",
    "    tv = result.get('time_varying', {})\n",
    "    if tv and 'comparison' in tv:\n",
    "        tv_data.append({\n",
    "            'market_id': result['market_id'],\n",
    "            'market_name': result.get('market_name', f\"Market {result['market_id']}\"),\n",
    "            'efficiency_change': tv['comparison']['efficiency_change'],\n",
    "            'early_more_inefficient': tv['comparison'].get('early_more_inefficient', False),\n",
    "            'late_more_inefficient': tv['comparison'].get('late_more_inefficient', False),\n",
    "            'volatility_ratio': tv['comparison'].get('volatility_ratio', 1)\n",
    "        })\n",
    "\n",
    "if tv_data:\n",
    "    tv_df = pd.DataFrame(tv_data)\n",
    "    \n",
    "    # Count markets in each category\n",
    "    efficiency_change_counts = tv_df['efficiency_change'].value_counts()\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = efficiency_change_counts.plot.bar(color=['green', 'gray', 'red'])\n",
    "    \n",
    "    plt.title('Efficiency Change Over Market Lifecycle')\n",
    "    plt.xlabel('Efficiency Change')\n",
    "    plt.ylabel('Number of Markets')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add percentage labels\n",
    "    total = len(tv_df)\n",
    "    for i, count in enumerate(efficiency_change_counts):\n",
    "        plt.text(i, count + 0.5, f\"{count/total*100:.1f}%\", ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569be738",
   "metadata": {},
   "source": [
    "## 7. Cross-Market Analysis (Market Relatedness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5df4422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_related_markets(event_id):\n",
    "    \"\"\"Find markets that are part of the same event\"\"\"\n",
    "    # Determine the event column\n",
    "    event_col = None\n",
    "    for col in ['event_id', 'groupId', 'group_id', 'event']:\n",
    "        if col in main_df.columns:\n",
    "            event_col = col\n",
    "            break\n",
    "    \n",
    "    if not event_col:\n",
    "        print(\"Could not find an event identifier column\")\n",
    "        return []\n",
    "    \n",
    "    # Get markets in this event\n",
    "    event_markets = main_df[main_df[event_col] == event_id]\n",
    "    if len(event_markets) <= 1:\n",
    "        return []\n",
    "    \n",
    "    return event_markets[id_column].unique().tolist()\n",
    "\n",
    "def analyze_cross_market_predictability(market_ids, max_lag=3):\n",
    "    \"\"\"Test for Granger causality between related markets\"\"\"\n",
    "    if len(market_ids) < 2:\n",
    "        return None\n",
    "    \n",
    "    # Process each market (using cached version)\n",
    "    market_data = {}\n",
    "    for market_id in market_ids:\n",
    "        data = preprocess_market_data_cached(market_id, resample='5min')  # Use wider intervals for cross-market\n",
    "        if data is not None and len(data) > max_lag + 5:\n",
    "            market_name = market_questions.get(str(market_id), f\"Market {market_id}\")\n",
    "            market_data[market_id] = {\n",
    "                'data': data,\n",
    "                'name': market_name\n",
    "            }\n",
    "    \n",
    "    if len(market_data) < 2:\n",
    "        return None\n",
    "    \n",
    "    # Pairwise Granger causality tests\n",
    "    causality_results = []\n",
    "    \n",
    "    for i, (market_i, data_i) in enumerate(market_data.items()):\n",
    "        for j, (market_j, data_j) in enumerate(market_data.items()):\n",
    "            if i >= j:  # Skip self-comparisons and duplicates\n",
    "                continue\n",
    "            \n",
    "            # Align time series\n",
    "            common_index = data_i['data'].index.intersection(data_j['data'].index)\n",
    "            if len(common_index) <= max_lag + 5:\n",
    "                continue\n",
    "                \n",
    "            series_i = data_i['data'].loc[common_index, 'price']\n",
    "            series_j = data_j['data'].loc[common_index, 'price']\n",
    "            \n",
    "            # Test if market i Granger-causes market j\n",
    "            try:\n",
    "                # i -> j\n",
    "                gc_result_ij = grangercausalitytests(\n",
    "                    pd.concat([series_j, series_i], axis=1), \n",
    "                    maxlag=max_lag, \n",
    "                    verbose=False\n",
    "                )\n",
    "                min_pvalue_ij = min([res[0]['ssr_chi2test'][1] for lag, res in gc_result_ij.items()])\n",
    "                \n",
    "                # j -> i\n",
    "                gc_result_ji = grangercausalitytests(\n",
    "                    pd.concat([series_i, series_j], axis=1), \n",
    "                    maxlag=max_lag, \n",
    "                    verbose=False\n",
    "                )\n",
    "                min_pvalue_ji = min([res[0]['ssr_chi2test'][1] for lag, res in gc_result_ji.items()])\n",
    "                \n",
    "                causality_results.append({\n",
    "                    'market_i_id': market_i,\n",
    "                    'market_j_id': market_j,\n",
    "                    'market_i_name': data_i['name'],\n",
    "                    'market_j_name': data_j['name'],\n",
    "                    'i_causes_j_pvalue': min_pvalue_ij,\n",
    "                    'j_causes_i_pvalue': min_pvalue_ji,\n",
    "                    'i_causes_j': min_pvalue_ij < 0.05,\n",
    "                    'j_causes_i': min_pvalue_ji < 0.05,\n",
    "                    'bidirectional': min_pvalue_ij < 0.05 and min_pvalue_ji < 0.05\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error in Granger causality test: {e}\")\n",
    "    \n",
    "    return causality_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a1795cc",
   "metadata": {},
   "source": [
    "Market Efficiency Analysis for Prediction Markets\n",
    "----------------------------------------------\n",
    "This notebook analyzes the efficiency of prediction markets using various statistical tests to evaluate\n",
    "whether these markets follow the \"wisdom of crowds\" hypothesis.\n",
    "## 1. Introduction & Research Questions\n",
    "\n",
    "This analysis aims to answer the following research questions:\n",
    "1. Do prediction markets on Polymarket exhibit weak-form efficiency?\n",
    "2. How does efficiency vary across different market types and contexts?\n",
    "3. Does efficiency change over a market's lifecycle?\n",
    "4. Can one market predict price movements in related markets?\n",
    "\n",
    "Efficient markets should have the following characteristics:\n",
    "- Non-stationary price series (random walk)\n",
    "- Stationary return series\n",
    "- No significant autocorrelation in returns\n",
    "- No significant predictability through AR models\n",
    "- Variance ratios close to 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eef993",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0973f190",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "from statsmodels.tsa.stattools import acf, pacf, adfuller\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "import warnings\n",
    "import json\n",
    "from scipy import stats\n",
    "\n",
    "# Try to use the notebook progress bar, fall back to terminal version if not available\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "except ImportError:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"viridis\")\n",
    "\n",
    "# Add the src directory to the path if it isn't already there\n",
    "if '../src' not in sys.path:\n",
    "    sys.path.append('../src')\n",
    "\n",
    "# Import utility functions\n",
    "from src.utils.data_loader import load_main_dataset, load_trade_data, get_sample_market_ids, load_market_question_mapping\n",
    "\n",
    "# Create output directory for saving results\n",
    "results_dir = 'results/knowledge_value/efficiency'\n",
    "os.makedirs(results_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71194dc",
   "metadata": {},
   "source": [
    " ## 3. Load and Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeff4d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "# Load the main dataset\n",
    "print(\"Loading main dataset...\")\n",
    "main_df = load_main_dataset('data/cleaned_election_data.csv')\n",
    "print(f\"Loaded dataset with {main_df.shape[0]} rows and {main_df.shape[1]} columns\")\n",
    "\n",
    "# Load market mapping\n",
    "market_questions = load_market_question_mapping('data/trades/market_id_to_question.json')\n",
    "print(f\"Loaded mapping for {len(market_questions)} markets\")\n",
    "\n",
    "# %%\n",
    "# Check column names to understand available data\n",
    "print(\"\\nColumn names in the dataset:\")\n",
    "print(main_df.columns.tolist())\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Determine ID Column for Markets\n",
    "\n",
    "# %%\n",
    "# Determine ID column\n",
    "id_column = None\n",
    "if 'market_id' in main_df.columns:\n",
    "    id_column = 'market_id'\n",
    "elif 'id' in main_df.columns:\n",
    "    id_column = 'id'\n",
    "else:\n",
    "    # Use the first column as ID\n",
    "    id_column = main_df.columns[0]\n",
    "    print(f\"Using {id_column} as the ID column\")\n",
    "\n",
    "print(f\"Using {id_column} as market identifier column\")\n",
    "\n",
    "# %%\n",
    "# Display some sample data\n",
    "print(\"\\nSample data:\")\n",
    "display(main_df.head())\n",
    "\n",
    "# %%\n",
    "# Get distribution of market types\n",
    "if 'event_electionType' in main_df.columns:\n",
    "    print(\"\\nDistribution of market types:\")\n",
    "    display(main_df['event_electionType'].value_counts())\n",
    "\n",
    "# %%\n",
    "# Get distribution by region/country\n",
    "if 'event_country' in main_df.columns:\n",
    "    print(\"\\nDistribution by country:\")\n",
    "    display(main_df['event_country'].value_counts().head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16e1b99",
   "metadata": {},
   "source": [
    "### 3.1 Select Markets for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31042c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of market IDs for analysis\n",
    "sort_column = 'volumeNum' if 'volumeNum' in main_df.columns else id_column\n",
    "sample_markets = main_df.sort_values(sort_column, ascending=False)[id_column].unique()\n",
    "\n",
    "# Limit the number of markets for initial analysis\n",
    "analysis_markets = sample_markets[:100]  # Adjust based on your computational resources\n",
    "print(f\"\\nSelected {len(analysis_markets)} markets for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8972dcf",
   "metadata": {},
   "source": [
    "## 4. Methodology & Implementation\n",
    "\n",
    "### 4.1 Market Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb383b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_market_data(market_id, resample='1min'):\n",
    "    \"\"\"\n",
    "    Convert raw trade data to time series of prices and returns.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    market_id : str\n",
    "        The ID of the market to analyze\n",
    "    resample : str\n",
    "        Frequency to resample the time series (default: '1min')\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with columns: timestamp, price, log_return\n",
    "    \"\"\"\n",
    "    # Load trade data for the specific market\n",
    "    trades_df = load_trade_data(market_id, trades_dir=\"data/trades\")\n",
    "    \n",
    "    if trades_df is None or len(trades_df) < 30:\n",
    "        print(f\"Insufficient trade data for market {market_id}\")\n",
    "        return None\n",
    "    \n",
    "    # Ensure timestamp is a datetime type\n",
    "    if not pd.api.types.is_datetime64_any_dtype(trades_df['timestamp']):\n",
    "        trades_df['timestamp'] = pd.to_datetime(trades_df['timestamp'])\n",
    "    \n",
    "    # Sort by timestamp\n",
    "    trades_df = trades_df.sort_values('timestamp')\n",
    "    \n",
    "    # Ensure price is numeric\n",
    "    if 'price' in trades_df.columns:\n",
    "        trades_df['price'] = pd.to_numeric(trades_df['price'], errors='coerce')\n",
    "    elif 'price_num' in trades_df.columns:\n",
    "        trades_df['price'] = pd.to_numeric(trades_df['price_num'], errors='coerce')\n",
    "    else:\n",
    "        print(f\"No price column found for market {market_id}\")\n",
    "        return None\n",
    "    \n",
    "    # Drop rows with NaN prices\n",
    "    trades_df = trades_df.dropna(subset=['price'])\n",
    "    \n",
    "    # Resample to regular intervals\n",
    "    trades_df = trades_df.set_index('timestamp')\n",
    "    price_series = trades_df['price'].resample(resample).last()\n",
    "    \n",
    "    # Fill missing values using forward fill\n",
    "    price_series = price_series.ffill()\n",
    "    \n",
    "    # Calculate log returns\n",
    "    log_returns = np.log(price_series / price_series.shift(1))\n",
    "    \n",
    "    # Create DataFrame\n",
    "    result_df = pd.DataFrame({\n",
    "        'price': price_series,\n",
    "        'log_return': log_returns\n",
    "    })\n",
    "    \n",
    "    # Drop rows with NaN\n",
    "    result_df = result_df.dropna()\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Create a cache for market data\n",
    "market_data_cache = {}\n",
    "\n",
    "def preprocess_market_data_cached(market_id, resample='1min'):\n",
    "    \"\"\"\n",
    "    Cached version of preprocess_market_data that stores results for reuse\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    market_id : str\n",
    "        The ID of the market to analyze\n",
    "    resample : str\n",
    "        Frequency to resample the time series (default: '1min')\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with columns: timestamp, price, log_return\n",
    "    \"\"\"\n",
    "    # Check if already in cache\n",
    "    cache_key = (market_id, resample)\n",
    "    if cache_key in market_data_cache:\n",
    "        return market_data_cache[cache_key]\n",
    "    \n",
    "    # If not in cache, process and store\n",
    "    result = preprocess_market_data(market_id, resample)\n",
    "    if result is not None:\n",
    "        market_data_cache[cache_key] = result\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb46f2cd",
   "metadata": {},
   "source": [
    "### 4.2 Test a Single Market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43aa1577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the preprocessing function on one market\n",
    "test_market_id = analysis_markets[0]\n",
    "market_name = market_questions.get(str(test_market_id), f\"Market {test_market_id}\")\n",
    "print(f\"\\nTesting preprocessing on market: {market_name} (ID: {test_market_id})\")\n",
    "\n",
    "market_data = preprocess_market_data(test_market_id)\n",
    "\n",
    "if market_data is not None:\n",
    "    print(f\"Successfully processed market data with {len(market_data)} rows\")\n",
    "    display(market_data.head())\n",
    "    \n",
    "    # Plot price series\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(market_data.index, market_data['price'])\n",
    "    plt.title(f'Price Series for Market {test_market_id}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Price')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot return series\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(market_data.index, market_data['log_return'])\n",
    "    plt.title(f'Log Return Series for Market {test_market_id}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Log Return')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Failed to process market data. Let's try another market.\")\n",
    "    if len(analysis_markets) > 1:\n",
    "        test_market_id = analysis_markets[1]\n",
    "        market_name = market_questions.get(str(test_market_id), f\"Market {test_market_id}\")\n",
    "        print(f\"Trying market: {market_name} (ID: {test_market_id})\")\n",
    "        market_data = preprocess_market_data(test_market_id)\n",
    "        if market_data is not None:\n",
    "            print(f\"Successfully processed market data with {len(market_data)} rows\")\n",
    "            display(market_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecea7b6",
   "metadata": {},
   "source": [
    "\n",
    "### 4.3 Define Efficiency Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b8ad0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_adf_test(series, series_type='price'):\n",
    "    \"\"\"\n",
    "    Run Augmented Dickey-Fuller test for unit root.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    series : pd.Series\n",
    "        Time series to test\n",
    "    series_type : str\n",
    "        Type of series ('price' or 'return')\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with test results\n",
    "    \"\"\"\n",
    "    # Run ADF test\n",
    "    result = adfuller(series.dropna())\n",
    "    \n",
    "    # Format results\n",
    "    adf_result = {\n",
    "        'adf_statistic': result[0],\n",
    "        'pvalue': result[1],\n",
    "        'critical_values': result[4],\n",
    "        'is_stationary': result[1] < 0.05  # Reject unit root if p-value < 0.05\n",
    "    }\n",
    "    \n",
    "    return adf_result\n",
    "\n",
    "def run_autocorrelation_tests(returns, lags=10):\n",
    "    \"\"\"\n",
    "    Run ACF tests on return series.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    returns : pd.Series\n",
    "        Series of log returns\n",
    "    lags : int\n",
    "        Number of lags to test\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with ACF results and significance\n",
    "    \"\"\"\n",
    "    # Calculate ACF\n",
    "    acf_values = acf(returns, nlags=lags, fft=True)\n",
    "    \n",
    "    # Calculate significance threshold\n",
    "    significance_level = 1.96 / np.sqrt(len(returns))  # 95% confidence level\n",
    "    \n",
    "    # Check for significant autocorrelation\n",
    "    significant_lags = []\n",
    "    for i in range(1, len(acf_values)):  # Skip lag 0 (always 1)\n",
    "        if abs(acf_values[i]) > significance_level:\n",
    "            significant_lags.append(i)\n",
    "    \n",
    "    result = {\n",
    "        'acf_values': acf_values.tolist(),\n",
    "        'significant_lags': significant_lags,\n",
    "        'has_significant_autocorrelation': len(significant_lags) > 0\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "def run_variance_ratio_test(returns, periods=[1, 5, 15, 60]):\n",
    "    \"\"\"\n",
    "    Run variance ratio test to check if variance scales linearly with time.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    returns : pd.Series\n",
    "        Series of log returns\n",
    "    periods : list\n",
    "        List of periods to test\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with variance ratio results\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Calculate variance for base period\n",
    "    base_period = periods[0]\n",
    "    base_var = returns.var()\n",
    "    \n",
    "    for period in periods[1:]:\n",
    "        # Skip if we don't have enough data\n",
    "        if len(returns) < period * 10:\n",
    "            continue\n",
    "            \n",
    "        # Aggregate returns for longer period\n",
    "        agg_returns = returns.rolling(window=period).sum()\n",
    "        agg_returns = agg_returns.dropna()\n",
    "        \n",
    "        if len(agg_returns) <= 1:\n",
    "            continue\n",
    "            \n",
    "        # Calculate variance\n",
    "        period_var = agg_returns.var()\n",
    "        \n",
    "        # Calculate variance ratio\n",
    "        var_ratio = period_var / (period * base_var)\n",
    "        \n",
    "        # Random walk hypothesis: var_ratio should be close to 1\n",
    "        # Calculate z-statistic (simplified)\n",
    "        n = len(returns)\n",
    "        std_error = np.sqrt(2 * (2 * period - 1) * (period - 1) / (3 * period * n))\n",
    "        z_stat = (var_ratio - 1) / std_error\n",
    "        p_value = 2 * (1 - abs(np.exp(-0.5 * z_stat**2) / np.sqrt(2 * np.pi)))\n",
    "        \n",
    "        results[f\"{period}min\"] = {\n",
    "            'variance_ratio': var_ratio,\n",
    "            'z_statistic': z_stat,\n",
    "            'p_value': p_value,\n",
    "            'significant': p_value < 0.05,\n",
    "            'interpretation': 'Mean Reversion' if var_ratio < 1 else 'Momentum' if var_ratio > 1 else 'Random Walk'\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def run_runs_test(returns):\n",
    "    \"\"\"\n",
    "    Run a runs test to check for non-random patterns in returns.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    returns : pd.Series\n",
    "        Series of log returns\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with runs test results\n",
    "    \"\"\"\n",
    "    # Convert returns to binary sequence (1 for positive, 0 for negative)\n",
    "    binary_seq = (returns > 0).astype(int)\n",
    "    \n",
    "    # Count runs\n",
    "    runs = 1\n",
    "    for i in range(1, len(binary_seq)):\n",
    "        if binary_seq.iloc[i] != binary_seq.iloc[i-1]:  # Use iloc for positional indexing\n",
    "            runs += 1\n",
    "    \n",
    "    # Calculate expected runs and variance\n",
    "    n = len(binary_seq)\n",
    "    n1 = binary_seq.sum()  # Count of 1s\n",
    "    n0 = n - n1  # Count of 0s\n",
    "    \n",
    "    if n0 == 0 or n1 == 0:  # All returns are positive or negative\n",
    "        return {\n",
    "            'runs': runs,\n",
    "            'expected_runs': np.nan,\n",
    "            'z_statistic': np.nan,\n",
    "            'p_value': np.nan,\n",
    "            'is_random': False\n",
    "        }\n",
    "    \n",
    "    expected_runs = 1 + 2 * n1 * n0 / n\n",
    "    std_runs = np.sqrt(2 * n1 * n0 * (2 * n1 * n0 - n) / (n**2 * (n-1)))\n",
    "    \n",
    "    # Calculate z-statistic\n",
    "    z_stat = (runs - expected_runs) / std_runs\n",
    "    p_value = 2 * (1 - abs(np.exp(-0.5 * z_stat**2) / np.sqrt(2 * np.pi)))\n",
    "    \n",
    "    return {\n",
    "        'runs': runs,\n",
    "        'expected_runs': expected_runs,\n",
    "        'z_statistic': z_stat,\n",
    "        'p_value': p_value,\n",
    "        'is_random': p_value >= 0.05  # Null hypothesis is randomness\n",
    "    }\n",
    "\n",
    "def fit_ar_model(returns, lags=1):\n",
    "    \"\"\"\n",
    "    Fit AR model to return series and evaluate predictability.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    returns : pd.Series\n",
    "        Series of log returns\n",
    "    lags : int\n",
    "        Order of the AR model\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with model results\n",
    "    \"\"\"\n",
    "    if len(returns) <= lags + 2:\n",
    "        return None\n",
    "        \n",
    "    # Fit AR model\n",
    "    try:\n",
    "        model = AutoReg(returns, lags=lags)\n",
    "        model_fit = model.fit()\n",
    "        \n",
    "        # Extract coefficient and p-value\n",
    "        coef = model_fit.params[1] if len(model_fit.params) > 1 else 0\n",
    "        p_value = model_fit.pvalues[1] if len(model_fit.pvalues) > 1 else 1\n",
    "        \n",
    "        return {\n",
    "            'ar_coefficient': coef,\n",
    "            'p_value': p_value,\n",
    "            'significant': p_value < 0.05,\n",
    "            'aic': model_fit.aic,\n",
    "            'bic': model_fit.bic\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error fitting AR model: {e}\")\n",
    "        return None\n",
    "\n",
    "def analyze_time_varying_efficiency(returns):\n",
    "    \"\"\"\n",
    "    Analyze how efficiency changes over time by dividing the returns series \n",
    "    into early, middle, and late periods.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    returns : pd.Series\n",
    "        Series of log returns\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with time-varying efficiency results\n",
    "    \"\"\"\n",
    "    if len(returns) < 90:  # Need enough data to divide\n",
    "        return None\n",
    "    \n",
    "    # Divide into three periods\n",
    "    period_size = len(returns) // 3\n",
    "    early_returns = returns.iloc[:period_size]\n",
    "    mid_returns = returns.iloc[period_size:2*period_size]\n",
    "    late_returns = returns.iloc[2*period_size:]\n",
    "    \n",
    "    # Test each period\n",
    "    periods = {\n",
    "        'early': early_returns,\n",
    "        'middle': mid_returns,\n",
    "        'late': late_returns\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for period_name, period_returns in periods.items():\n",
    "        if len(period_returns) < 30:  # Skip if not enough data\n",
    "            continue\n",
    "        \n",
    "        # Calculate autocorrelation\n",
    "        acf_result = run_autocorrelation_tests(period_returns)\n",
    "        \n",
    "        # Fit AR model\n",
    "        ar_result = fit_ar_model(period_returns)\n",
    "        \n",
    "        results[period_name] = {\n",
    "            'significant_acf': acf_result.get('has_significant_autocorrelation', False),\n",
    "            'ar_model': ar_result,\n",
    "            'return_volatility': period_returns.std(),\n",
    "            'sample_size': len(period_returns)\n",
    "        }\n",
    "    \n",
    "    # Compare early vs late\n",
    "    if 'early' in results and 'late' in results:\n",
    "        early_ar_sig = results['early'].get('ar_model', {}).get('significant', False) if results['early'].get('ar_model') else False\n",
    "        late_ar_sig = results['late'].get('ar_model', {}).get('significant', False) if results['late'].get('ar_model') else False\n",
    "        \n",
    "        efficiency_change = 'No Change'\n",
    "        if early_ar_sig and not late_ar_sig:\n",
    "            efficiency_change = 'More Efficient'\n",
    "        elif not early_ar_sig and late_ar_sig:\n",
    "            efficiency_change = 'Less Efficient'\n",
    "        \n",
    "        volatility_ratio = results['late']['return_volatility'] / results['early']['return_volatility'] if results['early']['return_volatility'] > 0 else 1\n",
    "        \n",
    "        results['comparison'] = {\n",
    "            'efficiency_change': efficiency_change,\n",
    "            'volatility_ratio': volatility_ratio,\n",
    "            'early_more_inefficient': early_ar_sig and not late_ar_sig,\n",
    "            'late_more_inefficient': not early_ar_sig and late_ar_sig\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_cross_market_predictability(market_ids, max_lag=3):\n",
    "    \"\"\"Test for Granger causality between related markets\"\"\"\n",
    "    if len(market_ids) < 2:\n",
    "        return None\n",
    "    \n",
    "    # Process each market\n",
    "    market_data = {}\n",
    "    for market_id in market_ids:\n",
    "        data = preprocess_market_data(market_id, resample='5min')  # Use wider intervals for cross-market\n",
    "        if data is not None and len(data) > max_lag + 5:\n",
    "            market_name = market_questions.get(str(market_id), f\"Market {market_id}\")\n",
    "            market_data[market_id] = {\n",
    "                'data': data,\n",
    "                'name': market_name\n",
    "            }\n",
    "    \n",
    "    if len(market_data) < 2:\n",
    "        return None\n",
    "    \n",
    "    # Pairwise Granger causality tests\n",
    "    causality_results = []\n",
    "    \n",
    "    for i, (market_i, data_i) in enumerate(market_data.items()):\n",
    "        for j, (market_j, data_j) in enumerate(market_data.items()):\n",
    "            if i >= j:  # Skip self-comparisons and duplicates\n",
    "                continue\n",
    "            \n",
    "            # Align time series\n",
    "            common_index = data_i['data'].index.intersection(data_j['data'].index)\n",
    "            if len(common_index) <= max_lag + 5:\n",
    "                continue\n",
    "                \n",
    "            series_i = data_i['data'].loc[common_index, 'price']\n",
    "            series_j = data_j['data'].loc[common_index, 'price']\n",
    "            \n",
    "            # Test if market i Granger-causes market j\n",
    "            try:\n",
    "                # i -> j\n",
    "                gc_result_ij = grangercausalitytests(\n",
    "                    pd.concat([series_j, series_i], axis=1), \n",
    "                    maxlag=max_lag, \n",
    "                    verbose=False\n",
    "                )\n",
    "                min_pvalue_ij = min([res[0]['ssr_chi2test'][1] for lag, res in gc_result_ij.items()])\n",
    "                \n",
    "                # j -> i\n",
    "                gc_result_ji = grangercausalitytests(\n",
    "                    pd.concat([series_i, series_j], axis=1), \n",
    "                    maxlag=max_lag, \n",
    "                    verbose=False\n",
    "                )\n",
    "                min_pvalue_ji = min([res[0]['ssr_chi2test'][1] for lag, res in gc_result_ji.items()])\n",
    "                \n",
    "                causality_results.append({\n",
    "                    'market_i_id': market_i,\n",
    "                    'market_j_id': market_j,\n",
    "                    'market_i_name': data_i['name'],\n",
    "                    'market_j_name': data_j['name'],\n",
    "                    'i_causes_j_pvalue': min_pvalue_ij,\n",
    "                    'j_causes_i_pvalue': min_pvalue_ji,\n",
    "                    'i_causes_j': min_pvalue_ij < 0.05,\n",
    "                    'j_causes_i': min_pvalue_ji < 0.05,\n",
    "                    'bidirectional': min_pvalue_ij < 0.05 and min_pvalue_ji < 0.05\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error in Granger causality test: {e}\")\n",
    "    \n",
    "    return causality_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22274f0",
   "metadata": {},
   "source": [
    "### 4.4 Run Tests on Single Market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa3550b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run efficiency tests on the test market\n",
    "if market_data is not None and len(market_data) > 60:\n",
    "    print(\"\\nRunning efficiency tests on market:\", market_name)\n",
    "    \n",
    "    # ADF test on price\n",
    "    adf_price = run_adf_test(market_data['price'], 'price')\n",
    "    print(\"\\nADF test on price series:\")\n",
    "    print(f\"ADF Statistic: {adf_price['adf_statistic']:.4f}\")\n",
    "    print(f\"p-value: {adf_price['pvalue']:.4f}\")\n",
    "    print(f\"Is stationary: {adf_price['is_stationary']}\")\n",
    "    print(f\"Interpretation: {'Inefficient (not a random walk)' if adf_price['is_stationary'] else 'Efficient (random walk)'}\")\n",
    "    \n",
    "    # ADF test on returns\n",
    "    adf_return = run_adf_test(market_data['log_return'], 'return')\n",
    "    print(\"\\nADF test on return series:\")\n",
    "    print(f\"ADF Statistic: {adf_return['adf_statistic']:.4f}\")\n",
    "    print(f\"p-value: {adf_return['pvalue']:.4f}\")\n",
    "    print(f\"Is stationary: {adf_return['is_stationary']}\")\n",
    "    print(f\"Interpretation: {'Efficient' if adf_return['is_stationary'] else 'Inefficient'}\")\n",
    "    \n",
    "    # Autocorrelation test\n",
    "    acf_result = run_autocorrelation_tests(market_data['log_return'])\n",
    "    print(\"\\nAutocorrelation test:\")\n",
    "    print(f\"Significant lags: {acf_result['significant_lags']}\")\n",
    "    print(f\"Has significant autocorrelation: {acf_result['has_significant_autocorrelation']}\")\n",
    "    print(f\"Interpretation: {'Inefficient' if acf_result['has_significant_autocorrelation'] else 'Efficient'}\")\n",
    "    \n",
    "    # Variance ratio test\n",
    "    vr_result = run_variance_ratio_test(market_data['log_return'])\n",
    "    print(\"\\nVariance ratio test:\")\n",
    "    for period, result in vr_result.items():\n",
    "        print(f\"  {period}: Ratio = {result['variance_ratio']:.4f}, p-value = {result['p_value']:.4f}, Interpretation: {result['interpretation']}\")\n",
    "    \n",
    "    # Runs test\n",
    "    runs_result = run_runs_test(market_data['log_return'])\n",
    "    print(\"\\nRuns test:\")\n",
    "    print(f\"Runs: {runs_result['runs']:.0f}, Expected: {runs_result['expected_runs']:.2f}\")\n",
    "    print(f\"Z-statistic: {runs_result['z_statistic']:.4f}, p-value: {runs_result['p_value']:.4f}\")\n",
    "    print(f\"Is random: {runs_result['is_random']}\")\n",
    "    print(f\"Interpretation: {'Efficient' if runs_result['is_random'] else 'Inefficient'}\")\n",
    "    \n",
    "    # AR model\n",
    "    ar_result = fit_ar_model(market_data['log_return'])\n",
    "    if ar_result:\n",
    "        print(\"\\nAR(1) model:\")\n",
    "        print(f\"Coefficient: {ar_result['ar_coefficient']:.4f}\")\n",
    "        print(f\"p-value: {ar_result['p_value']:.4f}\")\n",
    "        print(f\"Significant: {ar_result['significant']}\")\n",
    "        print(f\"Interpretation: {'Inefficient' if ar_result['significant'] else 'Efficient'}\")\n",
    "    \n",
    "    # Time-varying efficiency\n",
    "    tv_result = analyze_time_varying_efficiency(market_data['log_return'])\n",
    "    if tv_result:\n",
    "        print(\"\\nTime-varying efficiency:\")\n",
    "        for period_name, period_data in tv_result.items():\n",
    "            if period_name != 'comparison':\n",
    "                print(f\"  {period_name.capitalize()}: Significant autocorrelation = {period_data.get('significant_acf', False)}\")\n",
    "                if 'ar_model' in period_data and period_data['ar_model']:\n",
    "                    print(f\"    AR coefficient = {period_data['ar_model']['ar_coefficient']:.4f}, significant = {period_data['ar_model']['significant']}\")\n",
    "        \n",
    "        if 'comparison' in tv_result:\n",
    "            print(f\"  Efficiency change: {tv_result['comparison']['efficiency_change']}\")\n",
    "            print(f\"  Volatility ratio (late/early): {tv_result['comparison']['volatility_ratio']:.2f}\")\n",
    "    \n",
    "    # Create plots\n",
    "    # ACF plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plot_acf(market_data['log_return'], lags=30, alpha=0.05)\n",
    "    plt.title(f'Autocorrelation of Returns - {market_name}')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Price and returns histograms\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    sns.histplot(market_data['price'], kde=True, ax=ax1)\n",
    "    ax1.set_title('Price Distribution')\n",
    "    ax1.set_xlabel('Price')\n",
    "    \n",
    "    sns.histplot(market_data['log_return'], kde=True, ax=ax2)\n",
    "    ax2.set_title('Log Return Distribution')\n",
    "    ax2.set_xlabel('Log Return')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Not enough data to run efficiency tests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856a0bdf",
   "metadata": {},
   "source": [
    "## 5. Comprehensive Efficiency Analysis of All Markets\n",
    "\n",
    "### 5.1 Analysis Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70860c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_market_efficiency(market_id):\n",
    "    \"\"\"\n",
    "    Run a comprehensive market efficiency analysis on a single market.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    market_id : str\n",
    "        ID of the market to analyze\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with efficiency results\n",
    "    \"\"\"\n",
    "    market_result = {'market_id': market_id}\n",
    "    market_name = market_questions.get(str(market_id), f\"Market {market_id}\")\n",
    "    market_result['market_name'] = market_name\n",
    "    \n",
    "    # Preprocess market data using cached version\n",
    "    market_data = preprocess_market_data_cached(market_id)\n",
    "    if market_data is None or len(market_data) < 30:\n",
    "        return None\n",
    "    \n",
    "    # Get market metadata\n",
    "    market_rows = main_df[main_df[id_column] == market_id]\n",
    "    if len(market_rows) == 0:\n",
    "        # Try string comparison\n",
    "        market_rows = main_df[main_df[id_column].astype(str) == str(market_id)]\n",
    "    \n",
    "    if len(market_rows) > 0:\n",
    "        row = market_rows.iloc[0]\n",
    "        \n",
    "        # Extract market information safely\n",
    "        if 'event_electionType' in row:\n",
    "            market_result['event_type'] = row['event_electionType']\n",
    "        if 'event_country' in row:\n",
    "            market_result['country'] = row['event_country']\n",
    "        if 'volumeNum' in row:\n",
    "            market_result['volume'] = row['volumeNum']\n",
    "        if 'market_duration_days' in row:\n",
    "            market_result['duration_days'] = row['market_duration_days']\n",
    "    \n",
    "    # Run ADF tests\n",
    "    market_result['adf_price'] = run_adf_test(market_data['price'], 'price')\n",
    "    market_result['adf_return'] = run_adf_test(market_data['log_return'], 'return')\n",
    "    \n",
    "    # Run autocorrelation tests\n",
    "    market_result['autocorrelation'] = run_autocorrelation_tests(market_data['log_return'])\n",
    "    \n",
    "    # Run variance ratio test\n",
    "    market_result['variance_ratio'] = run_variance_ratio_test(market_data['log_return'])\n",
    "    \n",
    "    # Run runs test\n",
    "    market_result['runs_test'] = run_runs_test(market_data['log_return'])\n",
    "    \n",
    "    # Fit AR model\n",
    "    market_result['ar_model'] = fit_ar_model(market_data['log_return'])\n",
    "    \n",
    "    # Run time-varying efficiency analysis\n",
    "    market_result['time_varying'] = analyze_time_varying_efficiency(market_data['log_return'])\n",
    "    \n",
    "    # Calculate overall efficiency score (0-100, higher = more efficient)\n",
    "    score = 0\n",
    "    max_points = 0\n",
    "    \n",
    "    # 1. Non-stationary price (random walk) = efficient\n",
    "    if 'adf_price' in market_result:\n",
    "        max_points += 1\n",
    "        if not market_result['adf_price']['is_stationary']:\n",
    "            score += 1\n",
    "    \n",
    "    # 2. Stationary returns = efficient\n",
    "    if 'adf_return' in market_result:\n",
    "        max_points += 1\n",
    "        if market_result['adf_return']['is_stationary']:\n",
    "            score += 1\n",
    "    \n",
    "    # 3. No significant autocorrelation = efficient\n",
    "    if 'autocorrelation' in market_result:\n",
    "        max_points += 1\n",
    "        if not market_result['autocorrelation']['has_significant_autocorrelation']:\n",
    "            score += 1\n",
    "    \n",
    "    # 4. Random runs test = efficient\n",
    "    if 'runs_test' in market_result:\n",
    "        max_points += 1\n",
    "        if market_result['runs_test']['is_random']:\n",
    "            score += 1\n",
    "    \n",
    "    # 5. No significant AR model = efficient\n",
    "    if 'ar_model' in market_result and market_result['ar_model']:\n",
    "        max_points += 1\n",
    "        if not market_result['ar_model']['significant']:\n",
    "            score += 1\n",
    "    \n",
    "    # 6. Variance ratio close to 1 = efficient\n",
    "    if 'variance_ratio' in market_result and market_result['variance_ratio']:\n",
    "        vr_count = 0\n",
    "        for period, result in market_result['variance_ratio'].items():\n",
    "            max_points += 0.5\n",
    "            vr_count += 0.5\n",
    "            if not result['significant']:  # Not significantly different from 1\n",
    "                score += 0.5\n",
    "    \n",
    "    # Calculate percentage\n",
    "    if max_points > 0:\n",
    "        efficiency_score = (score / max_points) * 100\n",
    "    else:\n",
    "        efficiency_score = 0\n",
    "    \n",
    "    market_result['efficiency_score'] = efficiency_score\n",
    "    \n",
    "    # Efficiency classification\n",
    "    if efficiency_score >= 80:\n",
    "        market_result['efficiency_class'] = 'Highly Efficient'\n",
    "    elif efficiency_score >= 60:\n",
    "        market_result['efficiency_class'] = 'Moderately Efficient'\n",
    "    elif efficiency_score >= 40:\n",
    "        market_result['efficiency_class'] = 'Slightly Inefficient'\n",
    "    else:\n",
    "        market_result['efficiency_class'] = 'Highly Inefficient'\n",
    "    \n",
    "    return market_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3422bff",
   "metadata": {},
   "source": [
    "### 5.2 Run Analysis on All Selected Markets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07adb740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_markets_efficiently(market_ids, n_jobs=4):\n",
    "    \"\"\"\n",
    "    Run market efficiency analysis in parallel with caching\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    market_ids : list\n",
    "        List of market IDs to analyze\n",
    "    n_jobs : int\n",
    "        Number of parallel jobs (use -1 for all cores, or a specific number)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        List of analysis results\n",
    "    \"\"\"\n",
    "    print(f\"Running analysis on {len(market_ids)} markets using {n_jobs} parallel workers...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Run in parallel\n",
    "    results = Parallel(n_jobs=n_jobs, verbose=10)(\n",
    "        delayed(analyze_market_efficiency)(market_id) for market_id in market_ids\n",
    "    )\n",
    "    \n",
    "    # Filter out None results\n",
    "    valid_results = [r for r in results if r is not None]\n",
    "    \n",
    "    # Calculate execution time\n",
    "    execution_time = time.time() - start_time\n",
    "    print(f\"Successfully analyzed {len(valid_results)} markets in {execution_time:.2f} seconds\")\n",
    "    \n",
    "    return valid_results\n",
    "\n",
    "# Specify number of parallel jobs to use\n",
    "# Use fewer cores if memory is limited\n",
    "n_jobs = 4  # Adjust based on your machine's capabilities\n",
    "\n",
    "# Run parallel analysis\n",
    "print(f\"Running comprehensive efficiency analysis on {len(analysis_markets)} markets...\")\n",
    "efficiency_results = analyze_markets_efficiently(analysis_markets, n_jobs=n_jobs)\n",
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "results_df = pd.DataFrame(efficiency_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17620ad3",
   "metadata": {},
   "source": [
    "### 5.3 Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64201c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary of results\n",
    "print(\"\\nEfficiency Analysis Summary:\")\n",
    "print(f\"Total markets analyzed: {len(efficiency_results)}\")\n",
    "\n",
    "# Create DataFrame if it doesn't exist yet\n",
    "if 'results_df' not in locals() or not isinstance(results_df, pd.DataFrame):\n",
    "    results_df = pd.DataFrame(efficiency_results)\n",
    "\n",
    "if len(results_df) > 0:\n",
    "    # Overall statistics\n",
    "    print(f\"\\nAverage efficiency score: {results_df['efficiency_score'].mean():.2f}/100\")\n",
    "    \n",
    "    # Efficiency classification counts\n",
    "    print(\"\\nEfficiency Classification Breakdown:\")\n",
    "    class_counts = results_df['efficiency_class'].value_counts()\n",
    "    for cls, count in class_counts.items():\n",
    "        print(f\"  {cls}: {count} markets ({count/len(results_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Test results breakdown\n",
    "    if 'adf_price' in results_df.columns:\n",
    "        price_stationary = sum(results_df['adf_price'].apply(lambda x: x['is_stationary']))\n",
    "        print(f\"\\nADF Test (Price): {price_stationary} markets ({price_stationary/len(results_df)*100:.1f}%) have stationary prices\")\n",
    "    \n",
    "    if 'adf_return' in results_df.columns:\n",
    "        return_stationary = sum(results_df['adf_return'].apply(lambda x: x['is_stationary']))\n",
    "        print(f\"ADF Test (Returns): {return_stationary} markets ({return_stationary/len(results_df)*100:.1f}%) have stationary returns\")\n",
    "    \n",
    "    if 'autocorrelation' in results_df.columns:\n",
    "        autocorrelation = sum(results_df['autocorrelation'].apply(lambda x: x['has_significant_autocorrelation']))\n",
    "        print(f\"Autocorrelation: {autocorrelation} markets ({autocorrelation/len(results_df)*100:.1f}%) have significant autocorrelation\")\n",
    "    \n",
    "    if 'runs_test' in results_df.columns:\n",
    "        random_runs = sum(results_df['runs_test'].apply(lambda x: x['is_random']))\n",
    "        print(f\"Runs Test: {random_runs} markets ({random_runs/len(results_df)*100:.1f}%) have random runs\")\n",
    "    \n",
    "    if 'ar_model' in results_df.columns:\n",
    "        ar_significant = sum(results_df['ar_model'].apply(lambda x: x.get('significant', False) if x else False))\n",
    "        print(f\"AR Model: {ar_significant} markets ({ar_significant/len(results_df)*100:.1f}%) have significant AR(1) coefficients\")\n",
    "else:\n",
    "    print(\"No results to summarize. Make sure the analysis completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4aea08",
   "metadata": {},
   "source": [
    "## 6. Data Visualization and Interpretation\n",
    "### 6.1 Distribution of Efficiency Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330acc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of efficiency scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(results_df['efficiency_score'], bins=20, kde=True)\n",
    "plt.axvline(x=results_df['efficiency_score'].mean(), color='red', linestyle='--', \n",
    "           label=f'Mean: {results_df[\"efficiency_score\"].mean():.2f}')\n",
    "plt.title('Distribution of Market Efficiency Scores')\n",
    "plt.xlabel('Efficiency Score (higher = more efficient)')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Efficiency classification pie chart\n",
    "plt.figure(figsize=(10, 8))\n",
    "results_df['efficiency_class'].value_counts().plot.pie(autopct='%1.1f%%', \n",
    "                                                     colors=sns.color_palette(\"viridis\", 4),\n",
    "                                                     startangle=90)\n",
    "plt.title('Market Efficiency Classification')\n",
    "plt.ylabel('')  # Hide ylabel\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5863cc",
   "metadata": {},
   "source": [
    "### 6.2 Efficiency by Market Characteristics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51711a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'event_type' in results_df.columns:\n",
    "    # Filter to include only market types with sufficient data\n",
    "    type_counts = results_df['event_type'].value_counts()\n",
    "    market_types = type_counts[type_counts >= 5].index.tolist()\n",
    "    \n",
    "    if market_types:\n",
    "        type_data = []\n",
    "        for market_type in market_types:\n",
    "            type_df = results_df[results_df['event_type'] == market_type]\n",
    "            type_data.append({\n",
    "                'Market Type': market_type,\n",
    "                'Average Efficiency': type_df['efficiency_score'].mean(),\n",
    "                'Count': len(type_df)\n",
    "            })\n",
    "        \n",
    "        type_df = pd.DataFrame(type_data).sort_values('Average Efficiency', ascending=False)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        bars = plt.bar(type_df['Market Type'], type_df['Average Efficiency'], color='skyblue')\n",
    "        \n",
    "        # Add count labels\n",
    "        for i, bar in enumerate(bars):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 1, \n",
    "                    f\"n={type_df['Count'].iloc[i]}\", \n",
    "                    ha='center', va='bottom')\n",
    "        \n",
    "        plt.axhline(y=results_df['efficiency_score'].mean(), color='red', linestyle='--', \n",
    "                   label=f'Overall Average: {results_df[\"efficiency_score\"].mean():.2f}')\n",
    "        \n",
    "        plt.title('Average Efficiency Score by Market Type')\n",
    "        plt.xlabel('Market Type')\n",
    "        plt.ylabel('Average Efficiency Score')\n",
    "        plt.legend()\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.ylim(0, 100)\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# %%\n",
    "# Efficiency by region/country\n",
    "if 'country' in results_df.columns:\n",
    "    # Filter to include only countries with sufficient data\n",
    "    country_counts = results_df['country'].value_counts()\n",
    "    countries = country_counts[country_counts >= 5].index.tolist()\n",
    "    \n",
    "    if countries:\n",
    "        country_data = []\n",
    "        for country in countries:\n",
    "            country_df = results_df[results_df['country'] == country]\n",
    "            country_data.append({\n",
    "                'Country': country,\n",
    "                'Average Efficiency': country_df['efficiency_score'].mean(),\n",
    "                'Count': len(country_df)\n",
    "            })\n",
    "        \n",
    "        country_df = pd.DataFrame(country_data).sort_values('Average Efficiency', ascending=False)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        bars = plt.bar(country_df['Country'], country_df['Average Efficiency'], color='lightgreen')\n",
    "        \n",
    "        # Add count labels\n",
    "        for i, bar in enumerate(bars):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 1, \n",
    "                    f\"n={country_df['Count'].iloc[i]}\", \n",
    "                    ha='center', va='bottom')\n",
    "        \n",
    "        plt.axhline(y=results_df['efficiency_score'].mean(), color='red', linestyle='--', \n",
    "                   label=f'Overall Average: {results_df[\"efficiency_score\"].mean():.2f}')\n",
    "        \n",
    "        plt.title('Average Efficiency Score by Country')\n",
    "        plt.xlabel('Country')\n",
    "        plt.ylabel('Average Efficiency Score')\n",
    "        plt.legend()\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.ylim(0, 100)\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4064d7",
   "metadata": {},
   "source": [
    "### 6.3 Efficiency vs Market Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e3bf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficiency vs Market Volume\n",
    "if 'volume' in results_df.columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(results_df['volume'], results_df['efficiency_score'], alpha=0.6)\n",
    "    plt.xscale('log')  # Use log scale for volume\n",
    "    plt.title('Efficiency Score vs Market Volume')\n",
    "    plt.xlabel('Trading Volume (log scale)')\n",
    "    plt.ylabel('Efficiency Score')\n",
    "    \n",
    "    # Add trend line\n",
    "    try:\n",
    "        z = np.polyfit(np.log10(results_df['volume']), results_df['efficiency_score'], 1)\n",
    "        p = np.poly1d(z)\n",
    "        plt.plot(sorted(results_df['volume']), \n",
    "                p(np.log10(sorted(results_df['volume']))), \n",
    "                \"r--\", linewidth=2)\n",
    "        \n",
    "        # Calculate correlation\n",
    "        corr = np.corrcoef(np.log10(results_df['volume']), results_df['efficiency_score'])[0, 1]\n",
    "        plt.text(0.05, 0.95, f\"Correlation: {corr:.3f}\", transform=plt.gca().transAxes)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# %%\n",
    "# Efficiency vs Market Duration\n",
    "if 'duration_days' in results_df.columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(results_df['duration_days'], results_df['efficiency_score'], alpha=0.6)\n",
    "    plt.title('Efficiency Score vs Market Duration')\n",
    "    plt.xlabel('Market Duration (days)')\n",
    "    plt.ylabel('Efficiency Score')\n",
    "    \n",
    "    # Add trend line\n",
    "    try:\n",
    "        z = np.polyfit(results_df['duration_days'], results_df['efficiency_score'], 1)\n",
    "        p = np.poly1d(z)\n",
    "        plt.plot(sorted(results_df['duration_days']), \n",
    "                p(sorted(results_df['duration_days'])), \n",
    "                \"r--\", linewidth=2)\n",
    "        \n",
    "        # Calculate correlation\n",
    "        corr = np.corrcoef(results_df['duration_days'], results_df['efficiency_score'])[0, 1]\n",
    "        plt.text(0.05, 0.95, f\"Correlation: {corr:.3f}\", transform=plt.gca().transAxes)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba363b01",
   "metadata": {},
   "source": [
    "### 6.4 Time-Varying Efficiency Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f674123b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze time-varying efficiency results\n",
    "tv_data = []\n",
    "\n",
    "for result in efficiency_results:\n",
    "    tv = result.get('time_varying', {})\n",
    "    if tv and 'comparison' in tv:\n",
    "        tv_data.append({\n",
    "            'market_id': result['market_id'],\n",
    "            'market_name': result.get('market_name', f\"Market {result['market_id']}\"),\n",
    "            'efficiency_change': tv['comparison']['efficiency_change'],\n",
    "            'early_more_inefficient': tv['comparison'].get('early_more_inefficient', False),\n",
    "            'late_more_inefficient': tv['comparison'].get('late_more_inefficient', False),\n",
    "            'volatility_ratio': tv['comparison'].get('volatility_ratio', 1)\n",
    "        })\n",
    "\n",
    "if tv_data:\n",
    "    tv_df = pd.DataFrame(tv_data)\n",
    "    \n",
    "    # Count markets in each category\n",
    "    efficiency_change_counts = tv_df['efficiency_change'].value_counts()\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = efficiency_change_counts.plot.bar(color=['green', 'gray', 'red'])\n",
    "    \n",
    "    plt.title('Efficiency Change Over Market Lifecycle')\n",
    "    plt.xlabel('Efficiency Change')\n",
    "    plt.ylabel('Number of Markets')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add percentage labels\n",
    "    total = len(tv_df)\n",
    "    for i, count in enumerate(efficiency_change_counts):\n",
    "        plt.text(i, count + 0.5, f\"{count/total*100:.1f}%\", ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569be738",
   "metadata": {},
   "source": [
    "## 7. Cross-Market Analysis (Market Relatedness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5df4422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_related_markets(event_id):\n",
    "    \"\"\"Find markets that are part of the same event\"\"\"\n",
    "    # Determine the event column\n",
    "    event_col = None\n",
    "    for col in ['event_id', 'groupId', 'group_id', 'event']:\n",
    "        if col in main_df.columns:\n",
    "            event_col = col\n",
    "            break\n",
    "    \n",
    "    if not event_col:\n",
    "        print(\"Could not find an event identifier column\")\n",
    "        return []\n",
    "    \n",
    "    # Get markets in this event\n",
    "    event_markets = main_df[main_df[event_col] == event_id]\n",
    "    if len(event_markets) <= 1:\n",
    "        return []\n",
    "    \n",
    "    return event_markets[id_column].unique().tolist()\n",
    "\n",
    "def analyze_cross_market_predictability(market_ids, max_lag=3):\n",
    "    \"\"\"Test for Granger causality between related markets\"\"\"\n",
    "    if len(market_ids) < 2:\n",
    "        return None\n",
    "    \n",
    "    # Process each market (using cached version)\n",
    "    market_data = {}\n",
    "    for market_id in market_ids:\n",
    "        data = preprocess_market_data_cached(market_id, resample='5min')  # Use wider intervals for cross-market\n",
    "        if data is not None and len(data) > max_lag + 5:\n",
    "            market_name = market_questions.get(str(market_id), f\"Market {market_id}\")\n",
    "            market_data[market_id] = {\n",
    "                'data': data,\n",
    "                'name': market_name\n",
    "            }\n",
    "    \n",
    "    if len(market_data) < 2:\n",
    "        return None\n",
    "    \n",
    "    # Pairwise Granger causality tests\n",
    "    causality_results = []\n",
    "    \n",
    "    for i, (market_i, data_i) in enumerate(market_data.items()):\n",
    "        for j, (market_j, data_j) in enumerate(market_data.items()):\n",
    "            if i >= j:  # Skip self-comparisons and duplicates\n",
    "                continue\n",
    "            \n",
    "            # Align time series\n",
    "            common_index = data_i['data'].index.intersection(data_j['data'].index)\n",
    "            if len(common_index) <= max_lag + 5:\n",
    "                continue\n",
    "                \n",
    "            series_i = data_i['data'].loc[common_index, 'price']\n",
    "            series_j = data_j['data'].loc[common_index, 'price']\n",
    "            \n",
    "            # Test if market i Granger-causes market j\n",
    "            try:\n",
    "                # i -> j\n",
    "                gc_result_ij = grangercausalitytests(\n",
    "                    pd.concat([series_j, series_i], axis=1), \n",
    "                    maxlag=max_lag, \n",
    "                    verbose=False\n",
    "                )\n",
    "                min_pvalue_ij = min([res[0]['ssr_chi2test'][1] for lag, res in gc_result_ij.items()])\n",
    "                \n",
    "                # j -> i\n",
    "                gc_result_ji = grangercausalitytests(\n",
    "                    pd.concat([series_i, series_j], axis=1), \n",
    "                    maxlag=max_lag, \n",
    "                    verbose=False\n",
    "                )\n",
    "                min_pvalue_ji = min([res[0]['ssr_chi2test'][1] for lag, res in gc_result_ji.items()])\n",
    "                \n",
    "                causality_results.append({\n",
    "                    'market_i_id': market_i,\n",
    "                    'market_j_id': market_j,\n",
    "                    'market_i_name': data_i['name'],\n",
    "                    'market_j_name': data_j['name'],\n",
    "                    'i_causes_j_pvalue': min_pvalue_ij,\n",
    "                    'j_causes_i_pvalue': min_pvalue_ji,\n",
    "                    'i_causes_j': min_pvalue_ij < 0.05,\n",
    "                    'j_causes_i': min_pvalue_ji < 0.05,\n",
    "                    'bidirectional': min_pvalue_ij < 0.05 and min_pvalue_ji < 0.05\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error in Granger causality test: {e}\")\n",
    "    \n",
    "    return causality_results# Market Efficiency Analysis for Prediction Markets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a1795cc",
   "metadata": {},
   "source": [
    "Market Efficiency Analysis for Prediction Markets\n",
    "----------------------------------------------\n",
    "This notebook analyzes the efficiency of prediction markets using various statistical tests to evaluate\n",
    "whether these markets follow the \"wisdom of crowds\" hypothesis.\n",
    "## 1. Introduction & Research Questions\n",
    "\n",
    "This analysis aims to answer the following research questions:\n",
    "1. Do prediction markets on Polymarket exhibit weak-form efficiency?\n",
    "2. How does efficiency vary across different market types and contexts?\n",
    "3. Does efficiency change over a market's lifecycle?\n",
    "4. Can one market predict price movements in related markets?\n",
    "\n",
    "Efficient markets should have the following characteristics:\n",
    "- Non-stationary price series (random walk)\n",
    "- Stationary return series\n",
    "- No significant autocorrelation in returns\n",
    "- No significant predictability through AR models\n",
    "- Variance ratios close to 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eef993",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0973f190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements and setup (keep the existing imports)\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.stattools import acf, pacf, adfuller\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "from typing import Dict, List, Optional, Union, Tuple\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"viridis\")\n",
    "\n",
    "# Add the src directory to the path if it isn't already there\n",
    "if '../src' not in sys.path:\n",
    "    sys.path.append('../src')\n",
    "\n",
    "# Import utility functions\n",
    "from src.utils.data_loader import load_main_dataset, load_trade_data, get_sample_market_ids, load_market_question_mapping\n",
    "\n",
    "# Create output directory for saving results\n",
    "results_dir = 'results/knowledge_value/efficiency'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Create a cache for market data to avoid reloading\n",
    "market_data_cache = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71194dc",
   "metadata": {},
   "source": [
    " ## 3. Load and Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeff4d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load main dataset more efficiently\n",
    "def load_data(data_path='data/cleaned_election_data.csv', verbose=True):\n",
    "    \"\"\"\n",
    "    Load the main dataset and prepare market mappings efficiently\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (main_df, id_column, market_questions)\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"Loading main dataset...\")\n",
    "    \n",
    "    # Load with optimized settings\n",
    "    main_df = load_main_dataset(data_path)\n",
    "    \n",
    "    # Determine ID column\n",
    "    id_column = None\n",
    "    for col in ['market_id', 'id']:\n",
    "        if col in main_df.columns:\n",
    "            id_column = col\n",
    "            break\n",
    "    \n",
    "    if id_column is None:\n",
    "        id_column = main_df.columns[0]\n",
    "        if verbose:\n",
    "            print(f\"Using {id_column} as market identifier column\")\n",
    "    elif verbose:\n",
    "        print(f\"Using {id_column} as market identifier column\")\n",
    "    \n",
    "    # Create mappings\n",
    "    try:\n",
    "        market_questions = dict(zip(main_df[id_column].astype(str), main_df['question']))\n",
    "        if verbose:\n",
    "            print(f\"Created mapping for {len(market_questions)} markets\")\n",
    "    except:\n",
    "        # Try loading from file if column doesn't exist\n",
    "        try:\n",
    "            market_questions = load_market_question_mapping()\n",
    "            if verbose:\n",
    "                print(f\"Loaded mapping from file with {len(market_questions)} markets\")\n",
    "        except:\n",
    "            market_questions = {}\n",
    "            if verbose:\n",
    "                print(\"Could not create market question mapping\")\n",
    "    \n",
    "    return main_df, id_column, market_questions\n",
    "\n",
    "# Load data\n",
    "main_df, id_column, market_questions = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16e1b99",
   "metadata": {},
   "source": [
    "### 3.1 Select Markets for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31042c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of market IDs for analysis\n",
    "sort_column = 'volumeNum' if 'volumeNum' in main_df.columns else id_column\n",
    "sample_markets = main_df.sort_values(sort_column, ascending=False)[id_column].unique()\n",
    "\n",
    "# Limit the number of markets for initial analysis\n",
    "analysis_markets = sample_markets[:10]  # Adjust based on your computational resources\n",
    "print(f\"\\nSelected {len(analysis_markets)} markets for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8972dcf",
   "metadata": {},
   "source": [
    "## 4. Efficiency Tests\n",
    "### 4.1 Market Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb383b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_market_data(market_id, trades_dir='data/trades', resample='1min', verbose=False):\n",
    "    \"\"\"\n",
    "    Efficiently convert raw trade data to time series of prices and returns.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    market_id : str or int\n",
    "        The ID of the market to analyze\n",
    "    trades_dir : str\n",
    "        Path to the trades directory\n",
    "    resample : str\n",
    "        Frequency to resample the time series (default: '1min')\n",
    "    verbose : bool\n",
    "        Whether to print detailed output\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with columns: price, log_return\n",
    "        or None if processing fails\n",
    "    \"\"\"\n",
    "    # Check cache first\n",
    "    cache_key = (str(market_id), resample)\n",
    "    if cache_key in market_data_cache:\n",
    "        return market_data_cache[cache_key]\n",
    "    \n",
    "    # Load trade data\n",
    "    trades_df = load_trade_data(market_id, trades_dir=trades_dir)\n",
    "    \n",
    "    if trades_df is None or len(trades_df) < 30:\n",
    "        if verbose:\n",
    "            print(f\"Insufficient trade data for market {market_id}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Ensure timestamp is datetime\n",
    "        if not pd.api.types.is_datetime64_any_dtype(trades_df['timestamp']):\n",
    "            trades_df['timestamp'] = pd.to_datetime(trades_df['timestamp'])\n",
    "        \n",
    "        # Sort by timestamp\n",
    "        trades_df = trades_df.sort_values('timestamp')\n",
    "        \n",
    "        # Find price column\n",
    "        price_col = None\n",
    "        for col in ['price', 'price_num']:\n",
    "            if col in trades_df.columns:\n",
    "                price_col = col\n",
    "                break\n",
    "        \n",
    "        if price_col is None:\n",
    "            if verbose:\n",
    "                print(f\"No price column found for market {market_id}\")\n",
    "            return None\n",
    "        \n",
    "        # Ensure price is numeric\n",
    "        trades_df[price_col] = pd.to_numeric(trades_df[price_col], errors='coerce')\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Processing market {market_id}\")\n",
    "            print(f\"Total trades: {len(trades_df)}\")\n",
    "            print(f\"Time span: {trades_df['timestamp'].min()} to {trades_df['timestamp'].max()}\")\n",
    "        \n",
    "        # Drop NaN prices and set timestamp as index\n",
    "        trades_df = trades_df.dropna(subset=[price_col])\n",
    "        trades_df = trades_df.set_index('timestamp')\n",
    "        \n",
    "        # Efficiently resample price series\n",
    "        price_series = trades_df[price_col].resample(resample).last()\n",
    "        price_series = price_series.ffill()\n",
    "        \n",
    "        # Calculate log returns\n",
    "        log_returns = np.log(price_series / price_series.shift(1))\n",
    "        \n",
    "        # Create result DataFrame and drop NaNs\n",
    "        result_df = pd.DataFrame({\n",
    "            'price': price_series,\n",
    "            'log_return': log_returns\n",
    "        }).dropna()\n",
    "        \n",
    "        # Cache the result\n",
    "        market_data_cache[cache_key] = result_df\n",
    "        \n",
    "        return result_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"Error processing market {market_id}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecea7b6",
   "metadata": {},
   "source": [
    "\n",
    "### 4.2 Define Efficiency Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b8ad0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_efficiency_tests(market_data, verbose=False):\n",
    "    \"\"\"\n",
    "    Run a complete suite of market efficiency tests on processed data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    market_data : pd.DataFrame\n",
    "        DataFrame with price and log_return columns\n",
    "    verbose : bool\n",
    "        Whether to print detailed output\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with test results\n",
    "    \"\"\"\n",
    "    if market_data is None or len(market_data) < 30:\n",
    "        return None\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # ADF tests\n",
    "    try:\n",
    "        # Price stationarity (random walk test)\n",
    "        price_adf = adfuller(market_data['price'].dropna())\n",
    "        results['adf_price'] = {\n",
    "            'adf_statistic': price_adf[0],\n",
    "            'pvalue': price_adf[1],\n",
    "            'critical_values': price_adf[4],\n",
    "            'is_stationary': price_adf[1] < 0.05\n",
    "        }\n",
    "        \n",
    "        # Return stationarity\n",
    "        return_adf = adfuller(market_data['log_return'].dropna())\n",
    "        results['adf_return'] = {\n",
    "            'adf_statistic': return_adf[0],\n",
    "            'pvalue': return_adf[1],\n",
    "            'critical_values': return_adf[4],\n",
    "            'is_stationary': return_adf[1] < 0.05\n",
    "        }\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"Error in ADF tests: {str(e)}\")\n",
    "    \n",
    "    # Autocorrelation tests\n",
    "    try:\n",
    "        returns = market_data['log_return']\n",
    "        acf_values = acf(returns, nlags=10, fft=True)\n",
    "        significance_level = 1.96 / np.sqrt(len(returns))\n",
    "        \n",
    "        significant_lags = [i for i in range(1, len(acf_values)) \n",
    "                         if abs(acf_values[i]) > significance_level]\n",
    "        \n",
    "        results['autocorrelation'] = {\n",
    "            'acf_values': acf_values.tolist(),\n",
    "            'significant_lags': significant_lags,\n",
    "            'has_significant_autocorrelation': len(significant_lags) > 0\n",
    "        }\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"Error in autocorrelation tests: {str(e)}\")\n",
    "    \n",
    "    # Runs test\n",
    "    try:\n",
    "        # Convert returns to binary sequence\n",
    "        binary_seq = (market_data['log_return'] > 0).astype(int)\n",
    "        \n",
    "        # Count runs\n",
    "        runs = 1\n",
    "        for i in range(1, len(binary_seq)):\n",
    "            if binary_seq.iloc[i] != binary_seq.iloc[i-1]:\n",
    "                runs += 1\n",
    "        \n",
    "        # Calculate expected runs\n",
    "        n = len(binary_seq)\n",
    "        n1 = binary_seq.sum()\n",
    "        n0 = n - n1\n",
    "        \n",
    "        if n0 > 0 and n1 > 0:\n",
    "            expected_runs = 1 + 2 * n1 * n0 / n\n",
    "            std_runs = np.sqrt(2 * n1 * n0 * (2 * n1 * n0 - n) / (n**2 * (n-1)))\n",
    "            \n",
    "            # Calculate z-statistic\n",
    "            z_stat = (runs - expected_runs) / std_runs\n",
    "            p_value = 2 * (1 - abs(np.exp(-0.5 * z_stat**2) / np.sqrt(2 * np.pi)))\n",
    "            \n",
    "            results['runs_test'] = {\n",
    "                'runs': runs,\n",
    "                'expected_runs': expected_runs,\n",
    "                'z_statistic': z_stat,\n",
    "                'p_value': p_value,\n",
    "                'is_random': p_value >= 0.05\n",
    "            }\n",
    "        else:\n",
    "            results['runs_test'] = {\n",
    "                'runs': runs,\n",
    "                'is_random': False\n",
    "            }\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"Error in runs test: {str(e)}\")\n",
    "    \n",
    "    # Variance ratio test\n",
    "    try:\n",
    "        returns = market_data['log_return']\n",
    "        base_var = returns.var()\n",
    "        vr_results = {}\n",
    "        \n",
    "        for period in [5, 15, 30]:\n",
    "            if len(returns) < period * 10:\n",
    "                continue\n",
    "                \n",
    "            # Aggregate returns\n",
    "            agg_returns = returns.rolling(window=period).sum().dropna()\n",
    "            \n",
    "            if len(agg_returns) <= 1:\n",
    "                continue\n",
    "                \n",
    "            # Calculate variance ratio\n",
    "            period_var = agg_returns.var()\n",
    "            var_ratio = period_var / (period * base_var)\n",
    "            \n",
    "            # Test significance\n",
    "            n = len(returns)\n",
    "            std_error = np.sqrt(2 * (2 * period - 1) * (period - 1) / (3 * period * n))\n",
    "            z_stat = (var_ratio - 1) / std_error\n",
    "            p_value = 2 * (1 - abs(np.exp(-0.5 * z_stat**2) / np.sqrt(2 * np.pi)))\n",
    "            \n",
    "            vr_results[f\"{period}min\"] = {\n",
    "                'variance_ratio': var_ratio,\n",
    "                'z_statistic': z_stat,\n",
    "                'p_value': p_value,\n",
    "                'significant': p_value < 0.05,\n",
    "                'interpretation': 'Mean Reversion' if var_ratio < 1 else 'Momentum' if var_ratio > 1 else 'Random Walk'\n",
    "            }\n",
    "        \n",
    "        results['variance_ratio'] = vr_results\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"Error in variance ratio test: {str(e)}\")\n",
    "    \n",
    "    # AR model\n",
    "    try:\n",
    "        returns = market_data['log_return']\n",
    "        if len(returns) > 5:\n",
    "            model = AutoReg(returns, lags=1)\n",
    "            model_fit = model.fit()\n",
    "            \n",
    "            # Extract coefficient and p-value\n",
    "            coef = model_fit.params[1] if len(model_fit.params) > 1 else 0\n",
    "            p_value = model_fit.pvalues[1] if len(model_fit.pvalues) > 1 else 1\n",
    "            \n",
    "            results['ar_model'] = {\n",
    "                'ar_coefficient': coef,\n",
    "                'p_value': p_value,\n",
    "                'significant': p_value < 0.05,\n",
    "                'aic': model_fit.aic,\n",
    "                'bic': model_fit.bic\n",
    "            }\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"Error in AR model: {str(e)}\")\n",
    "    \n",
    "    # Time-varying efficiency\n",
    "    try:\n",
    "        if len(market_data) < 90:\n",
    "            return results\n",
    "        \n",
    "        # Divide data into periods\n",
    "        n = len(market_data)\n",
    "        period_size = n // 3\n",
    "        \n",
    "        periods = {\n",
    "            'early': market_data['log_return'].iloc[:period_size],\n",
    "            'middle': market_data['log_return'].iloc[period_size:2*period_size],\n",
    "            'late': market_data['log_return'].iloc[2*period_size:]\n",
    "        }\n",
    "        \n",
    "        period_results = {}\n",
    "        for name, period_returns in periods.items():\n",
    "            if len(period_returns) < 30:\n",
    "                continue\n",
    "                \n",
    "            # Test for autocorrelation\n",
    "            period_acf = acf(period_returns, nlags=5, fft=True)\n",
    "            sig_level = 1.96 / np.sqrt(len(period_returns))\n",
    "            has_acf = any(abs(period_acf[i]) > sig_level for i in range(1, len(period_acf)))\n",
    "            \n",
    "            # AR model for period\n",
    "            try:\n",
    "                period_model = AutoReg(period_returns, lags=1)\n",
    "                period_fit = period_model.fit()\n",
    "                ar_coef = period_fit.params[1] if len(period_fit.params) > 1 else 0\n",
    "                ar_pval = period_fit.pvalues[1] if len(period_fit.pvalues) > 1 else 1\n",
    "                ar_significant = ar_pval < 0.05\n",
    "            except:\n",
    "                ar_coef = None\n",
    "                ar_pval = None\n",
    "                ar_significant = False\n",
    "            \n",
    "            period_results[name] = {\n",
    "                'significant_acf': has_acf,\n",
    "                'ar_coefficient': ar_coef,\n",
    "                'ar_pvalue': ar_pval,\n",
    "                'ar_significant': ar_significant,\n",
    "                'volatility': period_returns.std(),\n",
    "                'sample_size': len(period_returns)\n",
    "            }\n",
    "        \n",
    "        # Compare early vs late periods\n",
    "        if 'early' in period_results and 'late' in period_results:\n",
    "            early = period_results['early']\n",
    "            late = period_results['late']\n",
    "            \n",
    "            efficiency_change = 'No Change'\n",
    "            if early['ar_significant'] and not late['ar_significant']:\n",
    "                efficiency_change = 'More Efficient'\n",
    "            elif not early['ar_significant'] and late['ar_significant']:\n",
    "                efficiency_change = 'Less Efficient'\n",
    "                \n",
    "            volatility_ratio = late['volatility'] / early['volatility'] if early['volatility'] > 0 else 1\n",
    "            \n",
    "            period_results['comparison'] = {\n",
    "                'efficiency_change': efficiency_change,\n",
    "                'volatility_ratio': volatility_ratio,\n",
    "                'early_more_inefficient': early['ar_significant'] and not late['ar_significant'],\n",
    "                'late_more_inefficient': not early['ar_significant'] and late['ar_significant']\n",
    "            }\n",
    "        \n",
    "        results['time_varying'] = period_results\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"Error in time-varying efficiency analysis: {str(e)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def calculate_efficiency_score(test_results):\n",
    "    \"\"\"Calculate an overall efficiency score (0-100) from test results\"\"\"\n",
    "    if not test_results:\n",
    "        return 0\n",
    "    \n",
    "    score = 0\n",
    "    max_score = 0\n",
    "    \n",
    "    # Non-stationary price series (random walk)\n",
    "    if 'adf_price' in test_results:\n",
    "        max_score += 20\n",
    "        if not test_results['adf_price']['is_stationary']:\n",
    "            score += 20\n",
    "    \n",
    "    # Stationary returns\n",
    "    if 'adf_return' in test_results:\n",
    "        max_score += 20\n",
    "        if test_results['adf_return']['is_stationary']:\n",
    "            score += 20\n",
    "    \n",
    "    # No significant autocorrelation\n",
    "    if 'autocorrelation' in test_results:\n",
    "        max_score += 20\n",
    "        if not test_results['autocorrelation']['has_significant_autocorrelation']:\n",
    "            score += 20\n",
    "    \n",
    "    # Random runs test\n",
    "    if 'runs_test' in test_results:\n",
    "        max_score += 20\n",
    "        if test_results['runs_test'].get('is_random', False):\n",
    "            score += 20\n",
    "    \n",
    "    # No significant AR model\n",
    "    if 'ar_model' in test_results:\n",
    "        max_score += 20\n",
    "        if not test_results['ar_model'].get('significant', True):\n",
    "            score += 20\n",
    "    \n",
    "    # Calculate total score\n",
    "    if max_score > 0:\n",
    "        return (score / max_score) * 100\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22274f0",
   "metadata": {},
   "source": [
    "## Market Analysis\n",
    "### 5.1 Single Market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa3550b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_market_by_id_or_name(market_id_or_name, main_df, id_column, market_questions):\n",
    "    \"\"\"\n",
    "    Find a market by its ID or name\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    market_id_or_name : str or int\n",
    "        Market ID or partial name to search for\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (market_id, market_info) or (None, None) if not found\n",
    "    \"\"\"\n",
    "    # Try direct ID match\n",
    "    if isinstance(market_id_or_name, (int, float)) or market_id_or_name.isdigit():\n",
    "        market_id = int(market_id_or_name) if market_id_or_name.isdigit() else market_id_or_name\n",
    "        market_rows = main_df[main_df[id_column] == market_id]\n",
    "        if not market_rows.empty:\n",
    "            return market_id, market_rows.iloc[0]\n",
    "    \n",
    "    # Try string ID match\n",
    "    market_rows = main_df[main_df[id_column].astype(str) == str(market_id_or_name)]\n",
    "    if not market_rows.empty:\n",
    "        return market_id_or_name, market_rows.iloc[0]\n",
    "    \n",
    "    # Try partial name match\n",
    "    if 'question' in main_df.columns:\n",
    "        name_matches = main_df[main_df['question'].str.contains(str(market_id_or_name), case=False, na=False)]\n",
    "        if not name_matches.empty:\n",
    "            match = name_matches.iloc[0]\n",
    "            return match[id_column], match\n",
    "    \n",
    "    # Try partial match in market_questions values\n",
    "    for id, question in market_questions.items():\n",
    "        if str(market_id_or_name).lower() in question.lower():\n",
    "            market_rows = main_df[main_df[id_column].astype(str) == str(id)]\n",
    "            if not market_rows.empty:\n",
    "                return id, market_rows.iloc[0]\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "def analyze_specific_market(market_id_or_name, main_df=None, id_column=None, market_questions=None):\n",
    "    \"\"\"\n",
    "    Analyze a specific market identified by ID or name\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    market_id_or_name : str or int\n",
    "        Market ID or name to search for\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Market analysis results\n",
    "    \"\"\"\n",
    "    if main_df is None:\n",
    "        main_df, id_column, market_questions = load_data(verbose=False)\n",
    "    \n",
    "    # Find the market\n",
    "    market_id, market_info = get_market_by_id_or_name(market_id_or_name, main_df, id_column, market_questions)\n",
    "    \n",
    "    if market_id is None:\n",
    "        print(f\"No market found matching '{market_id_or_name}'\")\n",
    "        return None\n",
    "    \n",
    "    # Display market information\n",
    "    print(\"\\n🔍 Market Information\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Market ID: {market_id}\")\n",
    "    \n",
    "    # Get the market question\n",
    "    market_name = None\n",
    "    if 'question' in market_info:\n",
    "        market_name = market_info['question']\n",
    "    else:\n",
    "        market_name = market_questions.get(str(market_id), f\"Market {market_id}\")\n",
    "    \n",
    "    print(f\"Market Question: {market_name}\")\n",
    "    \n",
    "    # Display additional information if available\n",
    "    for col, label in [\n",
    "        ('event_electionType', 'Election Type'),\n",
    "        ('event_country', 'Country'),\n",
    "        ('volumeNum', 'Trading Volume'),\n",
    "        ('market_duration_days', 'Market Duration (days)')\n",
    "    ]:\n",
    "        if col in market_info and not pd.isna(market_info[col]):\n",
    "            print(f\"{label}: {market_info[col]}\")\n",
    "    \n",
    "    # Process the market data\n",
    "    print(\"\\nProcessing market data...\")\n",
    "    market_data = preprocess_market_data(market_id, verbose=True)\n",
    "    \n",
    "    if market_data is None:\n",
    "        print(\"❌ Failed to process market data\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"✅ Successfully processed market data with {len(market_data)} time points\")\n",
    "    \n",
    "    # Run efficiency tests\n",
    "    print(\"\\nRunning market efficiency tests...\")\n",
    "    test_results = run_efficiency_tests(market_data, verbose=True)\n",
    "    \n",
    "    if test_results is None:\n",
    "        print(\"❌ Failed to run efficiency tests\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate efficiency score\n",
    "    efficiency_score = calculate_efficiency_score(test_results)\n",
    "    \n",
    "    # Determine efficiency class\n",
    "    if efficiency_score >= 80:\n",
    "        efficiency_class = 'Highly Efficient'\n",
    "    elif efficiency_score >= 60:\n",
    "        efficiency_class = 'Moderately Efficient'\n",
    "    elif efficiency_score >= 40:\n",
    "        efficiency_class = 'Slightly Inefficient'\n",
    "    else:\n",
    "        efficiency_class = 'Highly Inefficient'\n",
    "    \n",
    "    print(f\"\\n📊 Market Efficiency Score: {efficiency_score:.2f}/100\")\n",
    "    print(f\"📈 Efficiency Classification: {efficiency_class}\")\n",
    "    \n",
    "    # Print detailed test results\n",
    "    print(\"\\n🔬 Detailed Test Results:\")\n",
    "    \n",
    "    if 'adf_price' in test_results:\n",
    "        is_random_walk = not test_results['adf_price']['is_stationary']\n",
    "        print(f\"Random Walk Test (Non-stationary prices): {'✅ Pass' if is_random_walk else '❌ Fail'}\")\n",
    "    \n",
    "    if 'adf_return' in test_results:\n",
    "        is_return_stationary = test_results['adf_return']['is_stationary']\n",
    "        print(f\"Return Stationarity Test: {'✅ Pass' if is_return_stationary else '❌ Fail'}\")\n",
    "    \n",
    "    if 'autocorrelation' in test_results:\n",
    "        no_autocorr = not test_results['autocorrelation']['has_significant_autocorrelation']\n",
    "        print(f\"No Significant Autocorrelation: {'✅ Pass' if no_autocorr else '❌ Fail'}\")\n",
    "    \n",
    "    if 'runs_test' in test_results:\n",
    "        is_random = test_results['runs_test'].get('is_random', False)\n",
    "        print(f\"Runs Test (Randomness): {'✅ Pass' if is_random else '❌ Fail'}\")\n",
    "    \n",
    "    if 'ar_model' in test_results:\n",
    "        no_ar = not test_results['ar_model'].get('significant', True)\n",
    "        print(f\"No Significant AR Model: {'✅ Pass' if no_ar else '❌ Fail'}\")\n",
    "    \n",
    "    # Create visualizations\n",
    "    print(\"\\nGenerating visualizations...\")\n",
    "    visualize_market(market_data, market_name, test_results)\n",
    "    \n",
    "    # Prepare final results\n",
    "    results = {\n",
    "        'market_id': market_id,\n",
    "        'market_name': market_name,\n",
    "        'test_results': test_results,\n",
    "        'efficiency_score': efficiency_score,\n",
    "        'efficiency_class': efficiency_class\n",
    "    }\n",
    "    \n",
    "    # Add market attributes\n",
    "    for col in ['event_electionType', 'event_country', 'volumeNum', 'market_duration_days']:\n",
    "        if col in market_info and not pd.isna(market_info[col]):\n",
    "            results[col] = market_info[col]\n",
    "    \n",
    "    return results\n",
    "\n",
    "def visualize_market(market_data, market_name, test_results=None):\n",
    "    \"\"\"Create visualizations for a specific market\"\"\"\n",
    "    # Create a 2x2 plot grid\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Price Series\n",
    "    axs[0, 0].plot(market_data.index, market_data['price'], linewidth=2)\n",
    "    axs[0, 0].set_title(f'Price Series: {market_name}', fontsize=14)\n",
    "    axs[0, 0].set_xlabel('Date', fontsize=12)\n",
    "    axs[0, 0].set_ylabel('Price', fontsize=12)\n",
    "    axs[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Log Returns\n",
    "    axs[0, 1].plot(market_data.index, market_data['log_return'], linewidth=1, color='green')\n",
    "    axs[0, 1].set_title(f'Log Returns: {market_name}', fontsize=14)\n",
    "    axs[0, 1].set_xlabel('Date', fontsize=12)\n",
    "    axs[0, 1].set_ylabel('Log Return', fontsize=12)\n",
    "    axs[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. ACF Plot\n",
    "    if test_results and 'autocorrelation' in test_results:\n",
    "        acf_values = test_results['autocorrelation']['acf_values']\n",
    "        significant = test_results['autocorrelation']['has_significant_autocorrelation']\n",
    "        \n",
    "        lags = range(len(acf_values))\n",
    "        axs[1, 0].bar(lags, acf_values, width=0.3)\n",
    "        \n",
    "        # Plot confidence intervals for hypothesis testing\n",
    "        ci = 1.96 / np.sqrt(len(market_data))\n",
    "        axs[1, 0].axhline(y=0, linestyle='-', color='black', linewidth=1)\n",
    "        axs[1, 0].axhline(y=ci, linestyle='--', color='red', linewidth=1, alpha=0.7)\n",
    "        axs[1, 0].axhline(y=-ci, linestyle='--', color='red', linewidth=1, alpha=0.7)\n",
    "        \n",
    "        title = f'Autocorrelation Function: {\"❌ Significant\" if significant else \"✅ Not Significant\"}'\n",
    "        axs[1, 0].set_title(title, fontsize=14)\n",
    "        axs[1, 0].set_xlabel('Lag', fontsize=12)\n",
    "        axs[1, 0].set_ylabel('ACF', fontsize=12)\n",
    "    else:\n",
    "        axs[1, 0].set_title('Autocorrelation Function: Not Available', fontsize=14)\n",
    "    \n",
    "    # 4. Price distribution\n",
    "    axs[1, 1].hist(market_data['price'], bins=30, alpha=0.7, density=True)\n",
    "    axs[1, 1].set_title(f'Price Distribution: {market_name}', fontsize=14)\n",
    "    axs[1, 1].set_xlabel('Price', fontsize=12)\n",
    "    axs[1, 1].set_ylabel('Density', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # If time-varying efficiency results are available, show those too\n",
    "    if test_results and 'time_varying' in test_results and 'comparison' in test_results['time_varying']:\n",
    "        comparison = test_results['time_varying']['comparison']\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        periods = ['Early', 'Middle', 'Late']\n",
    "        \n",
    "        # Extract volatility for each period\n",
    "        volatilities = []\n",
    "        for period in ['early', 'middle', 'late']:\n",
    "            if period in test_results['time_varying']:\n",
    "                volatilities.append(test_results['time_varying'][period]['volatility'])\n",
    "            else:\n",
    "                volatilities.append(np.nan)\n",
    "        \n",
    "        # Create the bar chart\n",
    "        bars = plt.bar(periods, volatilities, color=['blue', 'green', 'orange'])\n",
    "        \n",
    "        plt.title(f'Return Volatility by Market Period: {comparison[\"efficiency_change\"]}', fontsize=14)\n",
    "        plt.ylabel('Return Volatility', fontsize=12)\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Add efficiency change information\n",
    "        plt.figtext(0.5, 0.01, f'Efficiency Change: {comparison[\"efficiency_change\"]}', \n",
    "                   ha='center', fontsize=12, bbox={\"facecolor\":\"lightgray\", \"alpha\":0.5, \"pad\":5})\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3422bff",
   "metadata": {},
   "source": [
    "### 5.2 Run Analysis on All Selected Markets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07adb740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_multiple_markets(market_ids=None, max_markets=100, use_parallel=True, save_results=True):\n",
    "    \"\"\"\n",
    "    Analyze multiple markets efficiently with memory optimization\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    market_ids : list, optional\n",
    "        List of market IDs to analyze. If None, selects by volume.\n",
    "    max_markets : int\n",
    "        Maximum number of markets to analyze\n",
    "    use_parallel : bool\n",
    "        Whether to use parallel processing\n",
    "    save_results : bool\n",
    "        Whether to save results to disk\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with market efficiency results\n",
    "    \"\"\"\n",
    "    global main_df, id_column\n",
    "    \n",
    "    # If no specific markets are provided, select by volume\n",
    "    if market_ids is None:\n",
    "        # Select markets by volume if available\n",
    "        if 'volumeNum' in main_df.columns:\n",
    "            market_ids = main_df.sort_values('volumeNum', ascending=False)[id_column].unique()[:max_markets]\n",
    "        else:\n",
    "            # Fall back to random selection\n",
    "            market_ids = main_df[id_column].sample(min(max_markets, len(main_df))).values\n",
    "    else:\n",
    "        # Limit to max_markets\n",
    "        market_ids = market_ids[:max_markets]\n",
    "    \n",
    "    print(f\"Analyzing {len(market_ids)} markets...\")\n",
    "    \n",
    "    # Setup results storage\n",
    "    results = []\n",
    "    results_file = os.path.join(results_dir, 'market_efficiency_results.json')\n",
    "    \n",
    "    # Load any existing results\n",
    "    if save_results and os.path.exists(results_file):\n",
    "        try:\n",
    "            with open(results_file, 'r') as f:\n",
    "                existing_results = json.load(f)\n",
    "                \n",
    "            # Extract already processed market IDs\n",
    "            processed_ids = [r.get('market_id') for r in existing_results]\n",
    "            \n",
    "            # Filter out already processed markets\n",
    "            market_ids = [mid for mid in market_ids if mid not in processed_ids]\n",
    "            \n",
    "            results = existing_results\n",
    "            print(f\"Loaded {len(existing_results)} existing results, {len(market_ids)} markets remaining\")\n",
    "        except:\n",
    "            print(\"Could not load existing results, starting fresh\")\n",
    "    \n",
    "    # Helper function for single market analysis\n",
    "    def analyze_market(market_id):\n",
    "        try:\n",
    "            # Process market data\n",
    "            market_data = preprocess_market_data(market_id)\n",
    "            if market_data is None or len(market_data) < 30:\n",
    "                return None\n",
    "            \n",
    "            # Run efficiency tests\n",
    "            test_results = run_efficiency_tests(market_data)\n",
    "            if test_results is None:\n",
    "                return None\n",
    "            \n",
    "            # Calculate efficiency score\n",
    "            efficiency_score = calculate_efficiency_score(test_results)\n",
    "            \n",
    "            # Determine efficiency class\n",
    "            if efficiency_score >= 80:\n",
    "                efficiency_class = 'Highly Efficient'\n",
    "            elif efficiency_score >= 60:\n",
    "                efficiency_class = 'Moderately Efficient'\n",
    "            elif efficiency_score >= 40:\n",
    "                efficiency_class = 'Slightly Inefficient'\n",
    "            else:\n",
    "                efficiency_class = 'Highly Inefficient'\n",
    "            \n",
    "            # Get market metadata\n",
    "            market_info = {}\n",
    "            market_rows = main_df[main_df[id_column] == market_id]\n",
    "            if len(market_rows) == 0:\n",
    "                # Try string comparison\n",
    "                market_rows = main_df[main_df[id_column].astype(str) == str(market_id)]\n",
    "            \n",
    "            if len(market_rows) > 0:\n",
    "                row = market_rows.iloc[0]\n",
    "                for col in ['event_electionType', 'event_country', 'volumeNum', 'market_duration_days', 'question']:\n",
    "                    if col in row and not pd.isna(row[col]):\n",
    "                        market_info[col] = row[col]\n",
    "            \n",
    "            # Get market name\n",
    "            market_name = market_info.get('question', market_questions.get(str(market_id), f\"Market {market_id}\"))\n",
    "            \n",
    "            # Create result\n",
    "            result = {\n",
    "                'market_id': market_id,\n",
    "                'market_name': market_name,\n",
    "                'efficiency_score': efficiency_score,\n",
    "                'efficiency_class': efficiency_class\n",
    "            }\n",
    "            \n",
    "            # Add market metadata\n",
    "            for key, value in market_info.items():\n",
    "                result[key] = value\n",
    "            \n",
    "            # Add key test results (avoiding large nested structures)\n",
    "            if 'adf_price' in test_results:\n",
    "                result['price_stationary'] = test_results['adf_price']['is_stationary']\n",
    "            \n",
    "            if 'adf_return' in test_results:\n",
    "                result['return_stationary'] = test_results['adf_return']['is_stationary']\n",
    "            \n",
    "            if 'autocorrelation' in test_results:\n",
    "                result['has_autocorrelation'] = test_results['autocorrelation']['has_significant_autocorrelation']\n",
    "            \n",
    "            if 'runs_test' in test_results:\n",
    "                result['is_random'] = test_results['runs_test'].get('is_random', False)\n",
    "            \n",
    "            if 'ar_model' in test_results:\n",
    "                result['ar_significant'] = test_results['ar_model'].get('significant', False)\n",
    "                result['ar_coefficient'] = test_results['ar_model'].get('ar_coefficient', None)\n",
    "            \n",
    "            if 'time_varying' in test_results and 'comparison' in test_results['time_varying']:\n",
    "                result['efficiency_change'] = test_results['time_varying']['comparison']['efficiency_change']\n",
    "            \n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing market {market_id}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    # Analysis with progress tracking and incremental saving\n",
    "    if use_parallel and len(market_ids) > 10:\n",
    "        from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "        import multiprocessing\n",
    "        \n",
    "        # Determine number of workers\n",
    "        n_workers = max(1, min(multiprocessing.cpu_count() - 1, 4))  # Limit to 4 workers max\n",
    "        \n",
    "        with ProcessPoolExecutor(max_workers=n_workers) as executor:\n",
    "            # Submit jobs\n",
    "            futures = {executor.submit(analyze_market, mid): mid for mid in market_ids}\n",
    "            \n",
    "            # Process results as they complete\n",
    "            for i, future in enumerate(tqdm(as_completed(futures), total=len(futures), desc=\"Analyzing markets\")):\n",
    "                result = future.result()\n",
    "                if result is not None:\n",
    "                    results.append(result)\n",
    "                \n",
    "                # Save incrementally every 10 markets\n",
    "                if save_results and (i + 1) % 10 == 0:\n",
    "                    try:\n",
    "                        with open(results_file, 'w') as f:\n",
    "                            json.dump(results, f)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error saving intermediate results: {str(e)}\")\n",
    "    else:\n",
    "        # Serial processing\n",
    "        for i, market_id in enumerate(tqdm(market_ids, desc=\"Analyzing markets\")):\n",
    "            result = analyze_market(market_id)\n",
    "            if result is not None:\n",
    "                results.append(result)\n",
    "            \n",
    "            # Save incrementally every 10 markets\n",
    "            if save_results and (i + 1) % 10 == 0:\n",
    "                try:\n",
    "                    with open(results_file, 'w') as f:\n",
    "                        json.dump(results, f)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error saving intermediate results: {str(e)}\")\n",
    "    \n",
    "    # Final save\n",
    "    if save_results:\n",
    "        try:\n",
    "            with open(results_file, 'w') as f:\n",
    "                json.dump(results, f)\n",
    "            print(f\"Saved {len(results)} results to {results_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving final results: {str(e)}\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Print summary\n",
    "    if len(results_df) > 0:\n",
    "        print(\"\\n📊 Analysis Summary\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Total markets analyzed: {len(results_df)}\")\n",
    "        print(f\"Average efficiency score: {results_df['efficiency_score'].mean():.2f}/100\")\n",
    "        \n",
    "        # Efficiency classification breakdown\n",
    "        print(\"\\nEfficiency Classification:\")\n",
    "        for cls, count in results_df['efficiency_class'].value_counts().items():\n",
    "            print(f\"  {cls}: {count} markets ({count/len(results_df)*100:.1f}%)\")\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17620ad3",
   "metadata": {},
   "source": [
    "### 5.3 Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64201c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_efficiency_results(results_df, save_dir=None):\n",
    "    \"\"\"\n",
    "    Create visualizations for market efficiency results\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results_df : pd.DataFrame\n",
    "        DataFrame with market efficiency results\n",
    "    save_dir : str, optional\n",
    "        Directory to save plots, if None uses results_dir\n",
    "    \"\"\"\n",
    "    if results_df is None or len(results_df) == 0:\n",
    "        print(\"No results to visualize\")\n",
    "        return\n",
    "    \n",
    "    if save_dir is None:\n",
    "        save_dir = results_dir\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # 1. Efficiency Score Distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(results_df['efficiency_score'], bins=20, kde=True)\n",
    "    plt.axvline(x=results_df['efficiency_score'].mean(), color='red', linestyle='--', \n",
    "               label=f'Mean: {results_df[\"efficiency_score\"].mean():.2f}')\n",
    "    plt.title('Distribution of Market Efficiency Scores', fontsize=14)\n",
    "    plt.xlabel('Efficiency Score (0-100, higher = more efficient)', fontsize=12)\n",
    "    plt.ylabel('Count', fontsize=12)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.savefig(os.path.join(save_dir, 'efficiency_score_distribution.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Efficiency Classification Pie Chart\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    results_df['efficiency_class'].value_counts().plot.pie(autopct='%1.1f%%', \n",
    "                                                         colors=sns.color_palette(\"viridis\", 4),\n",
    "                                                         startangle=90)\n",
    "    plt.title('Market Efficiency Classification', fontsize=14)\n",
    "    plt.ylabel('')  # Hide ylabel\n",
    "    \n",
    "    plt.savefig(os.path.join(save_dir, 'efficiency_classification_pie.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # 3. Efficiency by Market Type (if available)\n",
    "    if 'event_electionType' in results_df.columns:\n",
    "        type_counts = results_df['event_electionType'].value_counts()\n",
    "        types_with_data = type_counts[type_counts >= 5].index.tolist()\n",
    "        \n",
    "        if types_with_data:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            \n",
    "            # Calculate average efficiency by type\n",
    "            type_data = []\n",
    "            for market_type in types_with_data:\n",
    "                type_df = results_df[results_df['event_electionType'] == market_type]\n",
    "                type_data.append({\n",
    "                    'Market Type': market_type,\n",
    "                    'Average Efficiency': type_df['efficiency_score'].mean(),\n",
    "                    'Count': len(type_df)\n",
    "                })\n",
    "            \n",
    "            type_df = pd.DataFrame(type_data).sort_values('Average Efficiency', ascending=False)\n",
    "            \n",
    "            # Create bar chart\n",
    "            bars = plt.bar(type_df['Market Type'], type_df['Average Efficiency'], color='lightgreen')\n",
    "            \n",
    "            # Add count labels\n",
    "            for i, bar in enumerate(bars):\n",
    "                plt.text(bar.get_x() + bar.get_width()/2, \n",
    "                        bar.get_height() + 1, \n",
    "                        f\"n={type_df['Count'].iloc[i]}\", \n",
    "                        ha='center', va='bottom', fontsize=10)\n",
    "            \n",
    "            plt.axhline(y=results_df['efficiency_score'].mean(), color='red', linestyle='--', \n",
    "                       label=f'Overall Average: {results_df[\"efficiency_score\"].mean():.2f}')\n",
    "            \n",
    "            plt.title('Average Efficiency Score by Market Type', fontsize=14)\n",
    "            plt.xlabel('Market Type', fontsize=12)\n",
    "            plt.ylabel('Average Efficiency Score', fontsize=12)\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.legend()\n",
    "            plt.ylim(0, 100)\n",
    "            plt.grid(axis='y', alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            plt.savefig(os.path.join(save_dir, 'efficiency_by_market_type.png'), dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "    \n",
    "    # 4. Efficiency by Country (if available)\n",
    "    if 'event_country' in results_df.columns:\n",
    "        country_counts = results_df['event_country'].value_counts()\n",
    "        countries_with_data = country_counts[country_counts >= 5].index.tolist()\n",
    "        \n",
    "        if countries_with_data:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            \n",
    "            country_data = []\n",
    "            for country in countries_with_data:\n",
    "                country_df = results_df[results_df['event_country'] == country]\n",
    "                country_data.append({\n",
    "                    'Country': country,\n",
    "                    'Average Efficiency': country_df['efficiency_score'].mean(),\n",
    "                    'Count': len(country_df)\n",
    "                })\n",
    "            \n",
    "            country_df = pd.DataFrame(country_data).sort_values('Average Efficiency', ascending=False)\n",
    "            \n",
    "            bars = plt.bar(country_df['Country'], country_df['Average Efficiency'], color='skyblue')\n",
    "            \n",
    "            for i, bar in enumerate(bars):\n",
    "                plt.text(bar.get_x() + bar.get_width()/2, \n",
    "                        bar.get_height() + 1, \n",
    "                        f\"n={country_df['Count'].iloc[i]}\", \n",
    "                        ha='center', va='bottom', fontsize=10)\n",
    "            \n",
    "            plt.axhline(y=results_df['efficiency_score'].mean(), color='red', linestyle='--', \n",
    "                       label=f'Overall Average: {results_df[\"efficiency_score\"].mean():.2f}')\n",
    "            \n",
    "            plt.title('Average Efficiency Score by Country', fontsize=14)\n",
    "            plt.xlabel('Country', fontsize=12)\n",
    "            plt.ylabel('Average Efficiency Score', fontsize=12)\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.legend()\n",
    "            plt.ylim(0, 100)\n",
    "            plt.grid(axis='y', alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            plt.savefig(os.path.join(save_dir, 'efficiency_by_country.png'), dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "    \n",
    "    # 5. Efficiency vs Volume (if available)\n",
    "    if 'volumeNum' in results_df.columns:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Use log scale for volume\n",
    "        plt.scatter(results_df['volumeNum'], results_df['efficiency_score'], alpha=0.6)\n",
    "        plt.xscale('log')\n",
    "        \n",
    "        # Add trend line\n",
    "        try:\n",
    "            z = np.polyfit(np.log10(results_df['volumeNum']), results_df['efficiency_score'], 1)\n",
    "            p = np.poly1d(z)\n",
    "            \n",
    "            # Create x range for line (in log space)\n",
    "            x_range = np.logspace(\n",
    "                np.log10(results_df['volumeNum'].min()), \n",
    "                np.log10(results_df['volumeNum'].max()), \n",
    "                100\n",
    "            )\n",
    "            \n",
    "            plt.plot(x_range, p(np.log10(x_range)), \"r--\", linewidth=2)\n",
    "            \n",
    "            # Calculate correlation\n",
    "            corr = np.corrcoef(np.log10(results_df['volumeNum']), results_df['efficiency_score'])[0, 1]\n",
    "            plt.text(0.05, 0.95, f\"Correlation: {corr:.3f}\", transform=plt.gca().transAxes,\n",
    "                    bbox=dict(facecolor='white', alpha=0.8))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        plt.title('Efficiency Score vs Trading Volume', fontsize=14)\n",
    "        plt.xlabel('Trading Volume (log scale)', fontsize=12)\n",
    "        plt.ylabel('Efficiency Score', fontsize=12)\n",
    "        plt.grid(alpha=0.3)\n",
    "        \n",
    "        plt.savefig(os.path.join(save_dir, 'efficiency_vs_volume.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    # 6. Time-varying efficiency results\n",
    "    if 'efficiency_change' in results_df.columns:\n",
    "        efficiency_changes = results_df['efficiency_change'].value_counts()\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        bars = plt.bar(efficiency_changes.index, efficiency_changes.values, color=['green', 'gray', 'red'])\n",
    "        \n",
    "        # Add percentage labels\n",
    "        total = len(results_df)\n",
    "        for i, (category, count) in enumerate(efficiency_changes.items()):\n",
    "            plt.text(i, count + 0.5, f\"{count/total*100:.1f}%\", ha='center', fontsize=12)\n",
    "        \n",
    "        plt.title('Efficiency Change Over Market Lifecycle', fontsize=14)\n",
    "        plt.ylabel('Number of Markets', fontsize=12)\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        plt.savefig(os.path.join(save_dir, 'time_varying_efficiency.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569be738",
   "metadata": {},
   "source": [
    "## 7. Cross-Market Analysis (Market Relatedness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1073a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_related_markets(event_identifier, main_df=None, id_column=None):\n",
    "    \"\"\"\n",
    "    Find markets that are part of the same event\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    event_identifier : str or int\n",
    "        Event ID or name to search for\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        List of market IDs in the same event\n",
    "    \"\"\"\n",
    "    if main_df is None:\n",
    "        main_df, id_column, _ = load_data(verbose=False)\n",
    "    \n",
    "    # Try to find the event ID column\n",
    "    event_col = None\n",
    "    for col in ['event_id', 'groupId', 'group_id', 'event']:\n",
    "        if col in main_df.columns:\n",
    "            event_col = col\n",
    "            break\n",
    "    \n",
    "    if event_col is None:\n",
    "        print(\"Could not find an event identifier column\")\n",
    "        return []\n",
    "    \n",
    "    # If an event ID is provided directly\n",
    "    event_markets = main_df[main_df[event_col] == event_identifier]\n",
    "    \n",
    "    # If it's a string, try searching event/question text\n",
    "    if len(event_markets) == 0 and isinstance(event_identifier, str):\n",
    "        # Try matching in event name or question\n",
    "        for col in ['question', 'event_name', 'eventName', 'name']:\n",
    "            if col in main_df.columns:\n",
    "                event_markets = main_df[main_df[col].str.contains(event_identifier, case=False, na=False)]\n",
    "                if len(event_markets) > 0:\n",
    "                    break\n",
    "    \n",
    "    if len(event_markets) == 0:\n",
    "        print(f\"No markets found for event '{event_identifier}'\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"Found {len(event_markets)} markets in event '{event_identifier}'\")\n",
    "    return event_markets[id_column].unique().tolist()\n",
    "\n",
    "def analyze_cross_market_predictability(market_ids, max_lag=3, verbose=False):\n",
    "    \"\"\"\n",
    "    Test for Granger causality between related markets\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    market_ids : list\n",
    "        List of market IDs to analyze\n",
    "    max_lag : int\n",
    "        Maximum lag for Granger causality test\n",
    "    verbose : bool\n",
    "        Whether to print detailed output\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        List of causality results between market pairs\n",
    "    \"\"\"\n",
    "    if len(market_ids) < 2:\n",
    "        print(\"Need at least 2 markets for cross-market analysis\")\n",
    "        return None\n",
    "    \n",
    "    # Process each market data\n",
    "    market_data = {}\n",
    "    for market_id in tqdm(market_ids, desc=\"Processing markets\", disable=not verbose):\n",
    "        data = preprocess_market_data(market_id, resample='5min')  # Use wider intervals for cross-market\n",
    "        \n",
    "        if data is not None and len(data) > max_lag + 5:\n",
    "            market_name = market_questions.get(str(market_id), f\"Market {market_id}\")\n",
    "            market_data[market_id] = {\n",
    "                'data': data,\n",
    "                'name': market_name\n",
    "            }\n",
    "    \n",
    "    if len(market_data) < 2:\n",
    "        print(\"Insufficient data for cross-market analysis\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Analyzing relationships between {len(market_data)} markets...\")\n",
    "    \n",
    "    # Pairwise Granger causality tests\n",
    "    causality_results = []\n",
    "    \n",
    "    for i, (market_i, data_i) in enumerate(market_data.items()):\n",
    "        for j, (market_j, data_j) in enumerate(market_data.items()):\n",
    "            if i >= j:  # Skip self-comparisons and duplicates\n",
    "                continue\n",
    "            \n",
    "            # Align time series\n",
    "            common_index = data_i['data'].index.intersection(data_j['data'].index)\n",
    "            if len(common_index) <= max_lag + 5:\n",
    "                if verbose:\n",
    "                    print(f\"Insufficient overlapping data for {market_i} and {market_j}\")\n",
    "                continue\n",
    "                \n",
    "            series_i = data_i['data'].loc[common_index, 'price']\n",
    "            series_j = data_j['data'].loc[common_index, 'price']\n",
    "            \n",
    "            # Test if market i Granger-causes market j\n",
    "            try:\n",
    "                # i -> j\n",
    "                gc_result_ij = grangercausalitytests(\n",
    "                    pd.concat([series_j, series_i], axis=1), \n",
    "                    maxlag=max_lag, \n",
    "                    verbose=False\n",
    "                )\n",
    "                min_pvalue_ij = min([res[0]['ssr_chi2test'][1] for lag, res in gc_result_ij.items()])\n",
    "                \n",
    "                # j -> i\n",
    "                gc_result_ji = grangercausalitytests(\n",
    "                    pd.concat([series_i, series_j], axis=1), \n",
    "                    maxlag=max_lag, \n",
    "                    verbose=False\n",
    "                )\n",
    "                min_pvalue_ji = min([res[0]['ssr_chi2test'][1] for lag, res in gc_result_ji.items()])\n",
    "                \n",
    "                result = {\n",
    "                    'market_i_id': market_i,\n",
    "                    'market_j_id': market_j,\n",
    "                    'market_i_name': data_i['name'],\n",
    "                    'market_j_name': data_j['name'],\n",
    "                    'i_causes_j_pvalue': min_pvalue_ij,\n",
    "                    'j_causes_i_pvalue': min_pvalue_ji,\n",
    "                    'i_causes_j': min_pvalue_ij < 0.05,\n",
    "                    'j_causes_i': min_pvalue_ji < 0.05,\n",
    "                    'bidirectional': min_pvalue_ij < 0.05 and min_pvalue_ji < 0.05,\n",
    "                    'relationship': 'Bidirectional' if min_pvalue_ij < 0.05 and min_pvalue_ji < 0.05 else\n",
    "                                  f\"{data_i['name']} -> {data_j['name']}\" if min_pvalue_ij < 0.05 else\n",
    "                                  f\"{data_j['name']} -> {data_i['name']}\" if min_pvalue_ji < 0.05 else\n",
    "                                  'No relationship'\n",
    "                }\n",
    "                \n",
    "                causality_results.append(result)\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"Relationship between {data_i['name']} and {data_j['name']}: {result['relationship']}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                if verbose:\n",
    "                    print(f\"Error in Granger causality test between {market_i} and {market_j}: {str(e)}\")\n",
    "    \n",
    "    # Print summary\n",
    "    significant_count = sum(1 for r in causality_results if r['i_causes_j'] or r['j_causes_i'])\n",
    "    bidirectional_count = sum(1 for r in causality_results if r['bidirectional'])\n",
    "    \n",
    "    print(f\"\\nCross-market analysis complete:\")\n",
    "    print(f\"  - Tested {len(causality_results)} market pairs\")\n",
    "    print(f\"  - Found {significant_count} significant relationships ({significant_count/len(causality_results)*100:.1f}%)\")\n",
    "    print(f\"  - Found {bidirectional_count} bidirectional relationships ({bidirectional_count/len(causality_results)*100:.1f}%)\")\n",
    "    \n",
    "    return causality_results\n",
    "\n",
    "def visualize_cross_market_results(causality_results, save_dir=None):\n",
    "    \"\"\"\n",
    "    Visualize cross-market predictability results\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    causality_results : list\n",
    "        List of causality results from analyze_cross_market_predictability\n",
    "    save_dir : str, optional\n",
    "        Directory to save plots, if None uses results_dir\n",
    "    \"\"\"\n",
    "    if causality_results is None or len(causality_results) == 0:\n",
    "        print(\"No results to visualize\")\n",
    "        return\n",
    "    \n",
    "    if save_dir is None:\n",
    "        save_dir = results_dir\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    results_df = pd.DataFrame(causality_results)\n",
    "    \n",
    "    # 1. Relationship type counts\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    relationship_counts = results_df['relationship'].apply(\n",
    "        lambda x: 'Bidirectional' if 'Bidirectional' in x else 'Unidirectional' if '->' in x else 'No relationship'\n",
    "    ).value_counts()\n",
    "    \n",
    "    colors = ['green', 'orange', 'gray']\n",
    "    bars = plt.bar(relationship_counts.index, relationship_counts.values, color=colors)\n",
    "    \n",
    "    # Add percentage labels\n",
    "    total = len(results_df)\n",
    "    for i, (category, count) in enumerate(relationship_counts.items()):\n",
    "        plt.text(i, count + 0.5, f\"{count/total*100:.1f}%\", ha='center', fontsize=12)\n",
    "    \n",
    "    plt.title('Cross-Market Relationships', fontsize=14)\n",
    "    plt.ylabel('Number of Market Pairs', fontsize=12)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.savefig(os.path.join(save_dir, 'cross_market_relationships.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Create network visualization if NetworkX is available\n",
    "    try:\n",
    "        import networkx as nx\n",
    "        \n",
    "        # Create directed graph\n",
    "        G = nx.DiGraph()\n",
    "        \n",
    "        # Add nodes\n",
    "        all_markets = set()\n",
    "        for result in causality_results:\n",
    "            all_markets.add((result['market_i_id'], result['market_i_name']))\n",
    "            all_markets.add((result['market_j_id'], result['market_j_name']))\n",
    "        \n",
    "        for market_id, market_name in all_markets:\n",
    "            G.add_node(market_id, name=market_name)\n",
    "        \n",
    "        # Add edges\n",
    "        for result in causality_results:\n",
    "            if result['i_causes_j']:\n",
    "                G.add_edge(result['market_i_id'], result['market_j_id'], \n",
    "                          weight=1-result['i_causes_j_pvalue'])\n",
    "            \n",
    "            if result['j_causes_i']:\n",
    "                G.add_edge(result['market_j_id'], result['market_i_id'],\n",
    "                          weight=1-result['j_causes_i_pvalue'])\n",
    "        \n",
    "        # Create plot\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        \n",
    "        # Use spring layout\n",
    "        pos = nx.spring_layout(G, k=0.5, iterations=100)\n",
    "        \n",
    "        # Draw nodes\n",
    "        nx.draw_networkx_nodes(G, pos, node_size=500, node_color='skyblue', alpha=0.8)\n",
    "        \n",
    "        # Draw edges\n",
    "        edges = G.edges(data=True)\n",
    "        weights = [d['weight']*3 for _, _, d in edges]  # Scale weights for visibility\n",
    "        nx.draw_networkx_edges(G, pos, width=weights, alpha=0.5, \n",
    "                              arrows=True, arrowsize=15)\n",
    "        \n",
    "        # Add labels\n",
    "        labels = {node: G.nodes[node]['name'].split()[-1] for node in G.nodes()}\n",
    "        nx.draw_networkx_labels(G, pos, labels=labels, font_size=10)\n",
    "        \n",
    "        plt.title('Cross-Market Information Flow Network', fontsize=16)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.savefig(os.path.join(save_dir, 'cross_market_network.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    except:\n",
    "        print(\"NetworkX not available, skipping network visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae40119a",
   "metadata": {},
   "source": [
    "## 8. User Interface Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c30537",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_and_analyze_market(market_identifier=None):\n",
    "    \"\"\"\n",
    "    UI function for the notebook to select and analyze a specific market\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    market_identifier : str or int, optional\n",
    "        Market ID or name to analyze. If None, presents a selection interface.\n",
    "    \"\"\"\n",
    "    global main_df, id_column, market_questions\n",
    "    \n",
    "    if market_identifier is None:\n",
    "        # List top markets by volume for selection\n",
    "        if 'volumeNum' in main_df.columns:\n",
    "            top_markets = main_df.sort_values('volumeNum', ascending=False).head(10)\n",
    "        else:\n",
    "            top_markets = main_df.head(10)\n",
    "        \n",
    "        print(\"Please select a market by entering its ID or name, or choose from these top markets:\")\n",
    "        for i, (_, row) in enumerate(top_markets.iterrows()):\n",
    "            market_id = row[id_column]\n",
    "            market_name = row['question'] if 'question' in row else market_questions.get(str(market_id), f\"Market {market_id}\")\n",
    "            print(f\"{i+1}. ID: {market_id} - {market_name}\")\n",
    "        \n",
    "        # Let the user input their choice\n",
    "        choice = input(\"Enter market ID, name, or number from list: \")\n",
    "        \n",
    "        if choice.isdigit() and int(choice) <= len(top_markets):\n",
    "            # User selected by number from the list\n",
    "            market_id = top_markets.iloc[int(choice)-1][id_column]\n",
    "        else:\n",
    "            # User entered ID or name\n",
    "            market_identifier = choice\n",
    "    \n",
    "    # Analyze the selected market\n",
    "    result = analyze_specific_market(market_identifier)\n",
    "    return result\n",
    "\n",
    "def run_comprehensive_analysis(num_markets=100, use_parallel=True):\n",
    "    \"\"\"\n",
    "    UI function for the notebook to run comprehensive analysis on multiple markets\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    num_markets : int\n",
    "        Number of markets to analyze\n",
    "    use_parallel : bool\n",
    "        Whether to use parallel processing\n",
    "    \"\"\"\n",
    "    # Run the analysis\n",
    "    results_df = analyze_multiple_markets(max_markets=num_markets, use_parallel=use_parallel)\n",
    "    \n",
    "    # Visualize results\n",
    "    if results_df is not None and len(results_df) > 0:\n",
    "        visualize_efficiency_results(results_df)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def analyze_event_markets(event_identifier):\n",
    "    \"\"\"\n",
    "    UI function for the notebook to analyze cross-market relationships within an event\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    event_identifier : str or int\n",
    "        Event ID or name to analyze\n",
    "    \"\"\"\n",
    "    # Find related markets\n",
    "    related_markets = find_related_markets(event_identifier)\n",
    "    \n",
    "    if not related_markets or len(related_markets) < 2:\n",
    "        print(\"Insufficient related markets found for analysis\")\n",
    "        return None\n",
    "    \n",
    "    # Run cross-market analysis\n",
    "    results = analyze_cross_market_predictability(related_markets, verbose=True)\n",
    "    \n",
    "    # Visualize results\n",
    "    if results:\n",
    "        visualize_cross_market_results(results)\n",
    "    \n",
    "    return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
